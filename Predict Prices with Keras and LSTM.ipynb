{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from keras import Input\n",
    "from keras.engine import Model\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers import Concatenate, concatenate\n",
    "from keras.callbacks import TensorBoard\n",
    "\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# features is a list of strings of feature names \n",
    "\n",
    "def build_model(features, data_length, label_length):\n",
    "    \n",
    "    inputs_list = [] \n",
    "    for feature_name in features:\n",
    "        inputs_list.append((Input(shape=(data_length,1), name=feature_name)))\n",
    "    \n",
    "    layers = [] \n",
    "    for i, input_name in enumerate(inputs_list): \n",
    "        layers.append(LSTM(64, return_sequences=False)(inputs_list[i]) )\n",
    "        \n",
    "    output = concatenate(layers) \n",
    "    output = Dense(label_length, activation='linear', name='weighted_average_output')(output)\n",
    "    \n",
    "    model = Model(\n",
    "        inputs = inputs_list,\n",
    "        outputs = [output]\n",
    "    )\n",
    "    \n",
    "    model.compile(optimizer='rmsprop', loss='mse')\n",
    "    \n",
    "    return model    \n",
    "        \n",
    "data_length = 10\n",
    "label_length = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data [-5.44587603  5.27859267 -1.39306384 ..., -0.86772309  0.52915484\n",
      "  2.21562315]\n",
      "scaled [[ 0.27288984]\n",
      " [ 0.77679111]\n",
      " [ 0.4633158 ]\n",
      " ..., \n",
      " [ 0.48799952]\n",
      " [ 0.55363341]\n",
      " [ 0.63287403]]\n",
      "data [ 0.          0.07410797 -0.05505978 ...,  0.12921173  0.02985296  0.        ]\n",
      "scaled [[ 0.4536254 ]\n",
      " [ 0.47196961]\n",
      " [ 0.43999626]\n",
      " ..., \n",
      " [ 0.48560964]\n",
      " [ 0.46101501]\n",
      " [ 0.4536254 ]]\n",
      "data [ 0.00593818 -0.0164329  -0.00176565 ..., -0.00139514  0.03764151\n",
      " -0.00345164]\n",
      "scaled [[ 0.56657518]\n",
      " [ 0.48455698]\n",
      " [ 0.53833093]\n",
      " ..., \n",
      " [ 0.53968931]\n",
      " [ 0.68280789]\n",
      " [ 0.53214965]]\n",
      "data [ 0.01039096  0.00593818 -0.0164329  ..., -0.00379381 -0.00139514\n",
      "  0.03764151]\n",
      "scaled [[ 0.58290022]\n",
      " [ 0.56657518]\n",
      " [ 0.48455698]\n",
      " ..., \n",
      " [ 0.53089518]\n",
      " [ 0.53968931]\n",
      " [ 0.68280789]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Shoya\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\__main__.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Volume_BTC</th>\n",
       "      <th>Bitcoin_Adj</th>\n",
       "      <th>Close</th>\n",
       "      <th>Price_lagged</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.272890</td>\n",
       "      <td>0.453625</td>\n",
       "      <td>0.566575</td>\n",
       "      <td>0.582900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.776791</td>\n",
       "      <td>0.471970</td>\n",
       "      <td>0.484557</td>\n",
       "      <td>0.566575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.463316</td>\n",
       "      <td>0.439996</td>\n",
       "      <td>0.538331</td>\n",
       "      <td>0.484557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.725079</td>\n",
       "      <td>0.529463</td>\n",
       "      <td>0.520715</td>\n",
       "      <td>0.538331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.210661</td>\n",
       "      <td>0.416611</td>\n",
       "      <td>0.566098</td>\n",
       "      <td>0.520715</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Volume_BTC  Bitcoin_Adj     Close  Price_lagged\n",
       "0    0.272890     0.453625  0.566575      0.582900\n",
       "1    0.776791     0.471970  0.484557      0.566575\n",
       "2    0.463316     0.439996  0.538331      0.484557\n",
       "3    0.725079     0.529463  0.520715      0.538331\n",
       "4    0.210661     0.416611  0.566098      0.520715"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "master_df = pd.read_csv('C:/Users/Shoya/surf/data/master_df.csv', encoding='latin1')\n",
    "df = master_df[['Timestamp', 'Close', 'Volume_(BTC)', 'Volume_(Currency)', 'Date(UTC)', 'Bitcoin (Adj.Overlap)', \n",
    "               'Close Price % Change', 'Close Price % Change (Abs)', 'Is Spike']]\n",
    "\n",
    "# lag price \n",
    "df['Price_lagged'] = df['Close'].shift(1)\n",
    "\n",
    "df = df.dropna()\n",
    "\n",
    "df['Volume_BTC'] = df['Volume_(BTC)']\n",
    "df['Bitcoin_Adj'] = df['Bitcoin (Adj.Overlap)']\n",
    "\n",
    "cols = ['Volume_BTC','Bitcoin_Adj', 'Close', 'Price_lagged']\n",
    "\n",
    "# Stationalize Data by taking log differences\n",
    "data_array = np.diff(np.log(df[cols]), axis=0)\n",
    "\n",
    "# Min-Max Scale \n",
    "\n",
    "scalers = {}\n",
    "datas = [] \n",
    "\n",
    "df_scaled = pd.DataFrame(columns=cols)\n",
    "\n",
    "for i in range(len(cols)): \n",
    "    scalers[cols[i]] = MinMaxScaler()\n",
    "    print('data', data_array[:,i])\n",
    "    \n",
    "    col_data = data_array[:,i]\n",
    "    col_data = np.reshape(col_data, (len(col_data), 1))\n",
    "    \n",
    "    data = scalers[cols[i]].fit_transform( col_data )  #:, np.newaxis\n",
    "    print('scaled', data)\n",
    "    data = np.reshape(data, (1, len(data)))\n",
    "    df_scaled[cols[i]] = data[0]\n",
    "    \n",
    "\n",
    "# for data in datas:\n",
    "#     data_scaled = np.concatenate((data_scaled, data), axis=1)\n",
    "\n",
    "# scaler = MinMaxScaler() \n",
    "# data_scaled = scaler.fit_transform(data_array)\n",
    "# df_scaled = pd.DataFrame(data_scaled, columns=cols)\n",
    "\n",
    "display(df_scaled.head())\n",
    "\n",
    "# results in 0s ?? \n",
    "# scaler_y = MinMaxScaler()\n",
    "# scaler_y.fit_transform( [np.float32( np.diff(np.log(df['Close'] )  , axis=0)  )])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# cols = ['Timestamp','Volume_BTC', 'Bitcoin_Adj', 'Close']\n",
    "\n",
    "# for col in cols:\n",
    "#     df[col] = scaler.fit_transform([df[col].values])\n",
    "\n",
    "# X = df[['Timestamp', 'Volume_BTC', 'Bitcoin_Adj']].values\n",
    "# Y = df['Close'].values\n",
    "\n",
    "\n",
    "# X_train, X_test = X[1:train_size], X[train_size:len(X)]\n",
    "# Y_train, Y_test = Y[1:train_size], Y[train_size:len(X)]\n",
    "# print('Observations: %d' % (len(X)))\n",
    "# print('Training Observations: %d' % (len(X_train)))\n",
    "# print('Testing Observations: %d' % (len(Y_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split and reshape data to feed into RNN\n",
    "\n",
    "X_timestamp = df['Timestamp'].values\n",
    "X_volume = df['Volume_BTC'].values\n",
    "X_trends = df['Bitcoin_Adj'].values\n",
    "X_lagged_price = df['Price_lagged'].values\n",
    "\n",
    "Y_price = df['Close'].values\n",
    "\n",
    "train_size = int(len(X_timestamp) * 0.85)\n",
    "train_size = int(train_size/10) * 10 \n",
    "\n",
    "test_size_index = int(len(X_timestamp)/10)*10\n",
    "\n",
    "X_train_timestamp, X_test_timestamp = X_timestamp[:train_size], X_timestamp[train_size:test_size_index ]\n",
    "X_train_volume, X_test_volume = X_volume[:train_size], X_volume[train_size:test_size_index ]\n",
    "X_train_trends, X_test_trends = X_trends[:train_size], X_trends[train_size:test_size_index ]\n",
    "X_train_lagged_price, X_test_lagged_price = X_lagged_price[:train_size], X_lagged_price[train_size:test_size_index ]\n",
    "\n",
    "Y_train_price, Y_test_price = Y_price[:train_size], Y_price[train_size:test_size_index ]\n",
    "\n",
    "\n",
    "# X.shape is (samples, timesteps, dimension) \n",
    "# timestemps is 15, samples is just however many nobs there are (but it doesn't matter, so it should be None)\n",
    "\n",
    "\n",
    "\n",
    "X_train_timestamp = np.reshape(X_train_timestamp, (int(X_train_timestamp.shape[0]/data_length),data_length,1) ) \n",
    "X_train_volume = np.reshape(X_train_volume, (int(X_train_volume.shape[0]/data_length),data_length,1) ) \n",
    "X_train_trends = np.reshape(X_train_trends, (int(X_train_trends.shape[0]/data_length),data_length,1) ) \n",
    "X_train_lagged_price = np.reshape(X_train_lagged_price, (int(X_train_lagged_price.shape[0]/data_length), data_length, 1))\n",
    "\n",
    "X_test_timestamp = np.reshape(X_test_timestamp, (int(X_test_timestamp.shape[0]/data_length),data_length,1) ) \n",
    "X_test_volume = np.reshape(X_test_volume, (int(X_test_volume.shape[0]/data_length),data_length,1) ) \n",
    "X_test_trends = np.reshape(X_test_trends, (int(X_test_trends.shape[0]/data_length),data_length,1) )  \n",
    "X_test_lagged_price = np.reshape(X_test_lagged_price, (int(X_test_lagged_price.shape[0]/data_length),data_length,1))\n",
    "\n",
    "\n",
    "# Don't need the 1 for the third dimension for Y's??\n",
    "\n",
    "\n",
    "Y_train_price = np.reshape(Y_train_price, (int(Y_train_price.shape[0]/data_length),  data_length) ) \n",
    "Y_test_price = np.reshape(Y_test_price, (int(Y_test_price.shape[0]/data_length),  data_length) ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2386 samples, validate on 421 samples\n",
      "Epoch 1/120\n",
      "2386/2386 [==============================] - 3s - loss: 1799054.6865 - val_loss: 112387622.1188\n",
      "Epoch 2/120\n",
      "2386/2386 [==============================] - 3s - loss: 1784264.4891 - val_loss: 112232415.6580\n",
      "Epoch 3/120\n",
      "2386/2386 [==============================] - 3s - loss: 1770930.8477 - val_loss: 112081515.1734\n",
      "Epoch 4/120\n",
      "2386/2386 [==============================] - 3s - loss: 1757958.7420 - val_loss: 111935500.5986\n",
      "Epoch 5/120\n",
      "2386/2386 [==============================] - 3s - loss: 1744954.4524 - val_loss: 111783846.0048\n",
      "Epoch 6/120\n",
      "2386/2386 [==============================] - 3s - loss: 1732081.7298 - val_loss: 111634925.4727\n",
      "Epoch 7/120\n",
      "2386/2386 [==============================] - 3s - loss: 1719363.6318 - val_loss: 111485637.5867\n",
      "Epoch 8/120\n",
      "2386/2386 [==============================] - 3s - loss: 1706518.0897 - val_loss: 111336272.1140\n",
      "Epoch 9/120\n",
      "2386/2386 [==============================] - 3s - loss: 1694333.3471 - val_loss: 111190083.5534\n",
      "Epoch 10/120\n",
      "2386/2386 [==============================] - 3s - loss: 1681810.5958 - val_loss: 111039645.7767\n",
      "Epoch 11/120\n",
      "2386/2386 [==============================] - 3s - loss: 1669596.1178 - val_loss: 110893380.2185\n",
      "Epoch 12/120\n",
      "2386/2386 [==============================] - 3s - loss: 1657449.1237 - val_loss: 110745282.4323\n",
      "Epoch 13/120\n",
      "2386/2386 [==============================] - 3s - loss: 1645454.5208 - val_loss: 110597539.3064\n",
      "Epoch 14/120\n",
      "2386/2386 [==============================] - 3s - loss: 1633529.9059 - val_loss: 110448927.0499\n",
      "Epoch 15/120\n",
      "2386/2386 [==============================] - 3s - loss: 1621688.1542 - val_loss: 110302159.9240\n",
      "Epoch 16/120\n",
      "2386/2386 [==============================] - 3s - loss: 1609927.7148 - val_loss: 110154871.6010\n",
      "Epoch 17/120\n",
      "2386/2386 [==============================] - 3s - loss: 1598566.2300 - val_loss: 110008932.1235\n",
      "Epoch 18/120\n",
      "2386/2386 [==============================] - 3s - loss: 1586987.4383 - val_loss: 109860924.0475\n",
      "Epoch 19/120\n",
      "2386/2386 [==============================] - 3s - loss: 1575686.4053 - val_loss: 109714439.2209\n",
      "Epoch 20/120\n",
      "2386/2386 [==============================] - 3s - loss: 1564364.2368 - val_loss: 109566968.7981\n",
      "Epoch 21/120\n",
      "2386/2386 [==============================] - 3s - loss: 1553157.1957 - val_loss: 109418631.3159\n",
      "Epoch 22/120\n",
      "2386/2386 [==============================] - 3s - loss: 1542361.8962 - val_loss: 109274725.6437\n",
      "Epoch 23/120\n",
      "2386/2386 [==============================] - 3s - loss: 1531551.7626 - val_loss: 109129460.7126\n",
      "Epoch 24/120\n",
      "2386/2386 [==============================] - 3s - loss: 1520834.6591 - val_loss: 108985512.0190\n",
      "Epoch 25/120\n",
      "2386/2386 [==============================] - 3s - loss: 1510294.8566 - val_loss: 108840091.8385\n",
      "Epoch 26/120\n",
      "2386/2386 [==============================] - 3s - loss: 1499616.3710 - val_loss: 108693339.7055\n",
      "Epoch 27/120\n",
      "2386/2386 [==============================] - 3s - loss: 1489235.2062 - val_loss: 108548695.4489\n",
      "Epoch 28/120\n",
      "2386/2386 [==============================] - 3s - loss: 1479122.8474 - val_loss: 108404562.9834\n",
      "Epoch 29/120\n",
      "2386/2386 [==============================] - 3s - loss: 1468880.2609 - val_loss: 108258257.2922\n",
      "Epoch 30/120\n",
      "2386/2386 [==============================] - 3s - loss: 1458597.5408 - val_loss: 108114318.4038\n",
      "Epoch 31/120\n",
      "2386/2386 [==============================] - 3s - loss: 1448686.4849 - val_loss: 107968667.8385\n",
      "Epoch 32/120\n",
      "2386/2386 [==============================] - 3s - loss: 1438936.4885 - val_loss: 107824866.2993\n",
      "Epoch 33/120\n",
      "2386/2386 [==============================] - 3s - loss: 1429293.7741 - val_loss: 107682586.2993\n",
      "Epoch 34/120\n",
      "2386/2386 [==============================] - 3s - loss: 1419802.4119 - val_loss: 107539018.7933\n",
      "Epoch 35/120\n",
      "2386/2386 [==============================] - ETA: 0s - loss: 1416387.345 - 3s - loss: 1410197.4188 - val_loss: 107394977.2352\n",
      "Epoch 36/120\n",
      "2386/2386 [==============================] - 3s - loss: 1400738.5658 - val_loss: 107249914.4323\n",
      "Epoch 37/120\n",
      "2386/2386 [==============================] - 3s - loss: 1391657.2716 - val_loss: 107107434.7553\n",
      "Epoch 38/120\n",
      "2386/2386 [==============================] - 3s - loss: 1382208.3529 - val_loss: 106960766.4228\n",
      "Epoch 39/120\n",
      "2386/2386 [==============================] - 3s - loss: 1373512.9996 - val_loss: 106823525.1116\n",
      "Epoch 40/120\n",
      "2386/2386 [==============================] - 3s - loss: 1364532.3386 - val_loss: 106677806.9929\n",
      "Epoch 41/120\n",
      "2386/2386 [==============================] - 3s - loss: 1355804.9744 - val_loss: 106537859.6675\n",
      "Epoch 42/120\n",
      "2386/2386 [==============================] - 3s - loss: 1347220.7553 - val_loss: 106397444.5416\n",
      "Epoch 43/120\n",
      "2386/2386 [==============================] - 3s - loss: 1338742.7208 - val_loss: 106256014.1758\n",
      "Epoch 44/120\n",
      "2386/2386 [==============================] - 3s - loss: 1330278.7065 - val_loss: 106114873.7102\n",
      "Epoch 45/120\n",
      "2386/2386 [==============================] - 3s - loss: 1321783.1472 - val_loss: 105974555.6105\n",
      "Epoch 46/120\n",
      "2386/2386 [==============================] - 3s - loss: 1313711.5418 - val_loss: 105834164.1330\n",
      "Epoch 47/120\n",
      "2386/2386 [==============================] - 3s - loss: 1305887.6533 - val_loss: 105696949.0641\n",
      "Epoch 48/120\n",
      "2386/2386 [==============================] - 3s - loss: 1297912.0456 - val_loss: 105555496.1520\n",
      "Epoch 49/120\n",
      "2386/2386 [==============================] - 3s - loss: 1290085.2849 - val_loss: 105417066.5843\n",
      "Epoch 50/120\n",
      "2386/2386 [==============================] - 3s - loss: 1282332.3943 - val_loss: 105277668.8836\n",
      "Epoch 51/120\n",
      "2386/2386 [==============================] - 3s - loss: 1274736.6192 - val_loss: 105137194.8504\n",
      "Epoch 52/120\n",
      "2386/2386 [==============================] - 3s - loss: 1267187.3964 - val_loss: 104997594.8219\n",
      "Epoch 53/120\n",
      "2386/2386 [==============================] - 3s - loss: 1259817.0248 - val_loss: 104860038.9834\n",
      "Epoch 54/120\n",
      "2386/2386 [==============================] - 3s - loss: 1252465.9145 - val_loss: 104721439.3064\n",
      "Epoch 55/120\n",
      "2386/2386 [==============================] - 3s - loss: 1245329.7478 - val_loss: 104581772.7506\n",
      "Epoch 56/120\n",
      "2386/2386 [==============================] - 3s - loss: 1238293.2902 - val_loss: 104446081.1971\n",
      "Epoch 57/120\n",
      "2386/2386 [==============================] - 3s - loss: 1231407.9984 - val_loss: 104308155.5914\n",
      "Epoch 58/120\n",
      "2386/2386 [==============================] - 3s - loss: 1224585.7679 - val_loss: 104173601.8242\n",
      "Epoch 59/120\n",
      "2386/2386 [==============================] - 3s - loss: 1217744.7022 - val_loss: 104032788.9026\n",
      "Epoch 60/120\n",
      "2386/2386 [==============================] - 3s - loss: 1211351.9382 - val_loss: 103897462.9644\n",
      "Epoch 61/120\n",
      "2386/2386 [==============================] - 3s - loss: 1204618.5208 - val_loss: 103758847.9050\n",
      "Epoch 62/120\n",
      "2386/2386 [==============================] - 3s - loss: 1198289.6643 - val_loss: 103626480.9406\n",
      "Epoch 63/120\n",
      "2386/2386 [==============================] - 3s - loss: 1192168.0815 - val_loss: 103488921.5297\n",
      "Epoch 64/120\n",
      "2386/2386 [==============================] - 3s - loss: 1186150.8926 - val_loss: 103360493.1401\n",
      "Epoch 65/120\n",
      "2386/2386 [==============================] - 3s - loss: 1180326.4863 - val_loss: 103231370.6318\n",
      "Epoch 66/120\n",
      "2386/2386 [==============================] - 3s - loss: 1174453.8582 - val_loss: 103096695.5439\n",
      "Epoch 67/120\n",
      "2386/2386 [==============================] - 3s - loss: 1168598.6587 - val_loss: 102963082.2803\n",
      "Epoch 68/120\n",
      "2386/2386 [==============================] - 3s - loss: 1162975.0842 - val_loss: 102827438.1378\n",
      "Epoch 69/120\n",
      "2386/2386 [==============================] - 3s - loss: 1157373.1074 - val_loss: 102694939.4204\n",
      "Epoch 70/120\n",
      "2386/2386 [==============================] - 3s - loss: 1151805.0457 - val_loss: 102558566.2993\n",
      "Epoch 71/120\n",
      "2386/2386 [==============================] - 3s - loss: 1146540.1559 - val_loss: 102428623.6675\n",
      "Epoch 72/120\n",
      "2386/2386 [==============================] - 3s - loss: 1141297.5786 - val_loss: 102300693.4252\n",
      "Epoch 73/120\n",
      "2386/2386 [==============================] - 3s - loss: 1136286.2597 - val_loss: 102169878.0618\n",
      "Epoch 74/120\n",
      "2386/2386 [==============================] - 3s - loss: 1131223.3664 - val_loss: 102039222.8599\n",
      "Epoch 75/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2386/2386 [==============================] - 3s - loss: 1126308.8175 - val_loss: 101908290.4418\n",
      "Epoch 76/120\n",
      "2386/2386 [==============================] - 3s - loss: 1121655.1943 - val_loss: 101782052.1140\n",
      "Epoch 77/120\n",
      "2386/2386 [==============================] - 3s - loss: 1116856.6041 - val_loss: 101653603.9715\n",
      "Epoch 78/120\n",
      "2386/2386 [==============================] - 3s - loss: 1112501.2314 - val_loss: 101533694.4988\n",
      "Epoch 79/120\n",
      "2386/2386 [==============================] - 3s - loss: 1108120.8808 - val_loss: 101406119.5154\n",
      "Epoch 80/120\n",
      "2386/2386 [==============================] - 3s - loss: 1103874.5533 - val_loss: 101282837.1496\n",
      "Epoch 81/120\n",
      "2386/2386 [==============================] - 3s - loss: 1099722.7548 - val_loss: 101153769.1971\n",
      "Epoch 82/120\n",
      "2386/2386 [==============================] - 3s - loss: 1095565.5450 - val_loss: 101033130.6318\n",
      "Epoch 83/120\n",
      "2386/2386 [==============================] - 3s - loss: 1091654.9973 - val_loss: 100910030.1283\n",
      "Epoch 84/120\n",
      "2386/2386 [==============================] - 3s - loss: 1087763.1122 - val_loss: 100787278.5748\n",
      "Epoch 85/120\n",
      "2386/2386 [==============================] - 3s - loss: 1084039.0060 - val_loss: 100672340.6746\n",
      "Epoch 86/120\n",
      "2386/2386 [==============================] - 3s - loss: 1080414.9674 - val_loss: 100552393.2922\n",
      "Epoch 87/120\n",
      "2386/2386 [==============================] - 3s - loss: 1077064.9413 - val_loss: 100436033.4347\n",
      "Epoch 88/120\n",
      "2386/2386 [==============================] - 3s - loss: 1073530.6189 - val_loss: 100314296.8456\n",
      "Epoch 89/120\n",
      "2386/2386 [==============================] - 3s - loss: 1070230.8573 - val_loss: 100195081.6722\n",
      "Epoch 90/120\n",
      "2386/2386 [==============================] - 3s - loss: 1066899.7064 - val_loss: 100079688.8361\n",
      "Epoch 91/120\n",
      "2386/2386 [==============================] - 3s - loss: 1064010.5479 - val_loss: 99968811.1829\n",
      "Epoch 92/120\n",
      "2386/2386 [==============================] - 3s - loss: 1060984.3454 - val_loss: 99858465.9952\n",
      "Epoch 93/120\n",
      "2386/2386 [==============================] - 3s - loss: 1058139.3756 - val_loss: 99745746.0238\n",
      "Epoch 94/120\n",
      "2386/2386 [==============================] - 3s - loss: 1055246.2251 - val_loss: 99632412.2945\n",
      "Epoch 95/120\n",
      "2386/2386 [==============================] - 3s - loss: 1052626.7964 - val_loss: 99525727.6390\n",
      "Epoch 96/120\n",
      "2386/2386 [==============================] - 3s - loss: 1049873.3601 - val_loss: 99409480.4181\n",
      "Epoch 97/120\n",
      "2386/2386 [==============================] - 3s - loss: 1047345.4065 - val_loss: 99302605.8432\n",
      "Epoch 98/120\n",
      "2386/2386 [==============================] - 3s - loss: 1045000.4946 - val_loss: 99201344.1615\n",
      "Epoch 99/120\n",
      "2386/2386 [==============================] - 3s - loss: 1042718.7157 - val_loss: 99095976.4466\n",
      "Epoch 100/120\n",
      "2386/2386 [==============================] - 3s - loss: 1040475.9938 - val_loss: 98988619.8670\n",
      "Epoch 101/120\n",
      "2386/2386 [==============================] - 3s - loss: 1038217.4764 - val_loss: 98884449.5012\n",
      "Epoch 102/120\n",
      "2386/2386 [==============================] - 3s - loss: 1036156.7122 - val_loss: 98785726.8409\n",
      "Epoch 103/120\n",
      "2386/2386 [==============================] - 3s - loss: 1034281.2572 - val_loss: 98686260.8456\n",
      "Epoch 104/120\n",
      "2386/2386 [==============================] - 3s - loss: 1032334.6393 - val_loss: 98591693.8337\n",
      "Epoch 105/120\n",
      "2386/2386 [==============================] - 3s - loss: 1030599.2769 - val_loss: 98495798.1378\n",
      "Epoch 106/120\n",
      "2386/2386 [==============================] - 3s - loss: 1028935.2821 - val_loss: 98405793.5297\n",
      "Epoch 107/120\n",
      "2386/2386 [==============================] - 3s - loss: 1027231.1180 - val_loss: 98308772.3420\n",
      "Epoch 108/120\n",
      "2386/2386 [==============================] - 3s - loss: 1025704.6921 - val_loss: 98216245.8147\n",
      "Epoch 109/120\n",
      "2386/2386 [==============================] - 3s - loss: 1024164.7945 - val_loss: 98120823.2684\n",
      "Epoch 110/120\n",
      "2386/2386 [==============================] - 3s - loss: 1022619.1283 - val_loss: 98029194.7363\n",
      "Epoch 111/120\n",
      "2386/2386 [==============================] - 3s - loss: 1021280.9976 - val_loss: 97948894.2518\n",
      "Epoch 112/120\n",
      "2386/2386 [==============================] - 3s - loss: 1020136.8387 - val_loss: 97860893.8907\n",
      "Epoch 113/120\n",
      "2386/2386 [==============================] - 3s - loss: 1018821.1111 - val_loss: 97774782.1093\n",
      "Epoch 114/120\n",
      "2386/2386 [==============================] - 3s - loss: 1017652.8141 - val_loss: 97692364.2565\n",
      "Epoch 115/120\n",
      "2386/2386 [==============================] - 3s - loss: 1016652.8965 - val_loss: 97617478.5368\n",
      "Epoch 116/120\n",
      "2386/2386 [==============================] - 3s - loss: 1015589.5589 - val_loss: 97534790.5178\n",
      "Epoch 117/120\n",
      "2386/2386 [==============================] - 3s - loss: 1014622.8924 - val_loss: 97464720.0475\n",
      "Epoch 118/120\n",
      "2386/2386 [==============================] - 3s - loss: 1013823.4148 - val_loss: 97401408.9691\n",
      "Epoch 119/120\n",
      "2386/2386 [==============================] - 3s - loss: 1013022.7311 - val_loss: 97327367.5154\n",
      "Epoch 120/120\n",
      "2386/2386 [==============================] - 3s - loss: 1012265.1448 - val_loss: 97269843.2494\n"
     ]
    }
   ],
   "source": [
    "features = ['Volume_BTC', 'Bitcoin_Adj', 'Price_lagged']\n",
    "\n",
    "rnn = build_model(features, 10, 10) \n",
    "\n",
    "tensorboard_callback = TensorBoard(log_dir='./Graph', histogram_freq=0, write_graph=True, write_images=True)\n",
    "\n",
    "history = rnn.fit(\n",
    "    [\n",
    "        #X_train_timestamp,\n",
    "        X_train_volume,\n",
    "        X_train_trends,\n",
    "        X_train_lagged_price\n",
    "    ],\n",
    "    [\n",
    "        Y_train_price\n",
    "    ]\n",
    "    ,\n",
    "    validation_data=(\n",
    "        [\n",
    "            #X_test_timestamp,\n",
    "            X_test_volume,\n",
    "            X_test_trends,\n",
    "            X_test_lagged_price\n",
    "        ],\n",
    "        [\n",
    "            Y_test_price\n",
    "        ]),\n",
    "    epochs=120,\n",
    "    batch_size=32,\n",
    "    callbacks=[\n",
    "      tensorboard_callback\n",
    "    ],\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEDCAYAAADOc0QpAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x233ce460978>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# plot history\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='test')\n",
    "plt.legend()\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "320/421 [=====================>........] - ETA: 0s97269843.2494\n"
     ]
    }
   ],
   "source": [
    "score = rnn.evaluate(\n",
    "    [\n",
    "        #X_test_timestamp,\n",
    "        X_test_volume,\n",
    "        X_test_trends,\n",
    "        X_test_lagged_price\n",
    "    ],\n",
    "    [\n",
    "        Y_test_price\n",
    "    ])\n",
    "\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Shoya\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\__main__.py:17: RuntimeWarning: overflow encountered in exp\n",
      "C:\\Users\\Shoya\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\__main__.py:23: RuntimeWarning: overflow encountered in exp\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ inf,  inf,  inf, ...,  inf,  inf,  inf])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([ inf,  inf,  inf, ...,  inf,  inf,  inf], dtype=float32)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float64').",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-8a3f8e937645>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY_test_price_inv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0myhat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m \u001b[0mrmse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmean_squared_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY_test_price_inv\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0myhat\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Test RMSE: %.3f'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mrmse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\sklearn\\metrics\\regression.py\u001b[0m in \u001b[0;36mmean_squared_error\u001b[1;34m(y_true, y_pred, sample_weight, multioutput)\u001b[0m\n\u001b[0;32m    236\u001b[0m     \"\"\"\n\u001b[0;32m    237\u001b[0m     y_type, y_true, y_pred, multioutput = _check_reg_targets(\n\u001b[1;32m--> 238\u001b[1;33m         y_true, y_pred, multioutput)\n\u001b[0m\u001b[0;32m    239\u001b[0m     output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n\u001b[0;32m    240\u001b[0m                                weights=sample_weight)\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\sklearn\\metrics\\regression.py\u001b[0m in \u001b[0;36m_check_reg_targets\u001b[1;34m(y_true, y_pred, multioutput)\u001b[0m\n\u001b[0;32m     74\u001b[0m     \"\"\"\n\u001b[0;32m     75\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 76\u001b[1;33m     \u001b[0my_true\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     77\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    420\u001b[0m                              % (array.ndim, estimator_name))\n\u001b[0;32m    421\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 422\u001b[1;33m             \u001b[0m_assert_all_finite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    423\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    424\u001b[0m     \u001b[0mshape_repr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_shape_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[1;34m(X)\u001b[0m\n\u001b[0;32m     41\u001b[0m             and not np.isfinite(X).all()):\n\u001b[0;32m     42\u001b[0m         raise ValueError(\"Input contains NaN, infinity\"\n\u001b[1;32m---> 43\u001b[1;33m                          \" or a value too large for %r.\" % X.dtype)\n\u001b[0m\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float64')."
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt \n",
    "\n",
    "yhat = rnn.predict( \n",
    "    [\n",
    "        #X_test_timestamp,\n",
    "        X_test_volume,\n",
    "        X_test_trends,\n",
    "        X_test_lagged_price\n",
    "    ],\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# invert scaling \n",
    "inv_yhat = yhat.reshape((test_size_index - train_size, 1 ))# it forecasts the next 10 hours, so just reshape to 1D \n",
    "inv_yhat = scalers['Close'].inverse_transform(inv_yhat)  \n",
    "yhat = np.exp(np.cumsum(inv_yhat)) # undo log difference to get original \n",
    "\n",
    "# predictions are so off that it overflows to become infinites when I invert them back to their original scale \n",
    "\n",
    "Y_test_price = Y_test_price.reshape((test_size_index - train_size, 1 ))\n",
    "Y_test_price_inv = scalers['Close'].inverse_transform(Y_test_price) \n",
    "Y_test_price_inv = np.exp(np.cumsum( Y_test_price_inv   ))  \n",
    "\n",
    "#Y_test_price_inv = np.exp(np.cumsum(   np.concatenate( ( df['Close'].iloc[0]  ,Y_test_price_inv )  )   ))  # np.concatenate(([x[0]], x_diff))\n",
    "\n",
    "display(Y_test_price_inv, yhat)\n",
    "\n",
    "rmse = sqrt(mean_squared_error(Y_test_price_inv[1:], yhat[1:]))\n",
    "print('Test RMSE: %.3f' % rmse)\n",
    "\n",
    "plt.plot(Y_test_price_inv, label='actual')\n",
    "plt.plot(yhat, label='predicted')\n",
    "plt.legend()\n",
    "plt.show();\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# serialize model to JSON\n",
    "model_json = rnn.to_json()\n",
    "with open(\"model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "    \n",
    "# serialize weights to HDF5\n",
    "rnn.save_weights(\"model.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Probably need to change up x and y to frame it as a supervising problem -> look online to that one article\n",
    "# also scale it from 0 to 1 \n",
    "# remove seasonality \n",
    "# Change timestamp into some categorial input, not just a timestamp \n",
    "\n",
    "# look into if min-max scaling actually worked properly \n",
    "\n",
    "# Finally min-max scaled correctly so figure out how to unscale it (invert and undo log difference)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
