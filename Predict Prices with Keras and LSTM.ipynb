{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from keras import Input\n",
    "from keras.engine import Model\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers import Concatenate, concatenate\n",
    "from keras.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# features is a list of strings of feature names \n",
    "\n",
    "def build_model(features, data_length, label_length):\n",
    "    \n",
    "    inputs_list = [] \n",
    "    for feature_name in features:\n",
    "        inputs_list.append((Input(shape=(data_length,1), name=feature_name)))\n",
    "    \n",
    "    layers = [] \n",
    "    for i, input_name in enumerate(inputs_list): \n",
    "        layers.append(LSTM(64, return_sequences=False)(inputs_list[i]) )\n",
    "        \n",
    "    output = concatenate(layers) \n",
    "    output = Dense(label_length, activation='linear', name='weighted_average_output')(output)\n",
    "    \n",
    "    model = Model(\n",
    "        inputs = inputs_list,\n",
    "        outputs = [output]\n",
    "    )\n",
    "    \n",
    "    model.compile(optimizer='rmsprop', loss='mse')\n",
    "    \n",
    "    return model    \n",
    "        \n",
    "data_length = 10\n",
    "label_length = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Shoya\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\__main__.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data [-5.44587603  5.27859267 -1.39306384 ..., -0.86772309  0.52915484\n",
      "  2.21562315]\n",
      "scaled [[ 0.27288984]\n",
      " [ 0.77679111]\n",
      " [ 0.4633158 ]\n",
      " ..., \n",
      " [ 0.48799952]\n",
      " [ 0.55363341]\n",
      " [ 0.63287403]]\n",
      "data [ 0.          0.07410797 -0.05505978 ...,  0.12921173  0.02985296  0.        ]\n",
      "scaled [[ 0.4536254 ]\n",
      " [ 0.47196961]\n",
      " [ 0.43999626]\n",
      " ..., \n",
      " [ 0.48560964]\n",
      " [ 0.46101501]\n",
      " [ 0.4536254 ]]\n",
      "data [ 0.00593818 -0.0164329  -0.00176565 ..., -0.00139514  0.03764151\n",
      " -0.00345164]\n",
      "scaled [[ 0.56657518]\n",
      " [ 0.48455698]\n",
      " [ 0.53833093]\n",
      " ..., \n",
      " [ 0.53968931]\n",
      " [ 0.68280789]\n",
      " [ 0.53214965]]\n",
      "data [ 0.01039096  0.00593818 -0.0164329  ..., -0.00379381 -0.00139514\n",
      "  0.03764151]\n",
      "scaled [[ 0.58290022]\n",
      " [ 0.56657518]\n",
      " [ 0.48455698]\n",
      " ..., \n",
      " [ 0.53089518]\n",
      " [ 0.53968931]\n",
      " [ 0.68280789]]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Volume_BTC</th>\n",
       "      <th>Bitcoin_Adj</th>\n",
       "      <th>Close</th>\n",
       "      <th>Price_lagged</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.272890</td>\n",
       "      <td>0.453625</td>\n",
       "      <td>0.566575</td>\n",
       "      <td>0.582900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.776791</td>\n",
       "      <td>0.471970</td>\n",
       "      <td>0.484557</td>\n",
       "      <td>0.566575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.463316</td>\n",
       "      <td>0.439996</td>\n",
       "      <td>0.538331</td>\n",
       "      <td>0.484557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.725079</td>\n",
       "      <td>0.529463</td>\n",
       "      <td>0.520715</td>\n",
       "      <td>0.538331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.210661</td>\n",
       "      <td>0.416611</td>\n",
       "      <td>0.566098</td>\n",
       "      <td>0.520715</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Volume_BTC  Bitcoin_Adj     Close  Price_lagged\n",
       "0    0.272890     0.453625  0.566575      0.582900\n",
       "1    0.776791     0.471970  0.484557      0.566575\n",
       "2    0.463316     0.439996  0.538331      0.484557\n",
       "3    0.725079     0.529463  0.520715      0.538331\n",
       "4    0.210661     0.416611  0.566098      0.520715"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "master_df = pd.read_csv('C:/Users/Shoya/surf/data/master_df.csv', encoding='latin1')\n",
    "df = master_df[['Timestamp', 'Close', 'Volume_(BTC)', 'Volume_(Currency)', 'Date(UTC)', 'Bitcoin (Adj.Overlap)', \n",
    "               'Close Price % Change', 'Close Price % Change (Abs)', 'Is Spike']]\n",
    "\n",
    "# lag price \n",
    "df['Price_lagged'] = df['Close'].shift(1)\n",
    "\n",
    "df = df.dropna()\n",
    "\n",
    "df['Volume_BTC'] = df['Volume_(BTC)']\n",
    "df['Bitcoin_Adj'] = df['Bitcoin (Adj.Overlap)']\n",
    "\n",
    "cols = ['Volume_BTC','Bitcoin_Adj', 'Close', 'Price_lagged']\n",
    "\n",
    "# Stationalize Data by taking log differences\n",
    "data_array = np.diff(np.log(df[cols]), axis=0)\n",
    "\n",
    "# Min-Max Scale \n",
    "\n",
    "scalers = {}\n",
    "datas = [] \n",
    "\n",
    "df_scaled = pd.DataFrame(columns=cols)\n",
    "\n",
    "for i in range(len(cols)): \n",
    "    scalers[cols[i]] = MinMaxScaler()\n",
    "    print('data', data_array[:,i])\n",
    "    \n",
    "    col_data = data_array[:,i]\n",
    "    col_data = np.reshape(col_data, (len(col_data), 1))\n",
    "    \n",
    "    data = scalers[cols[i]].fit_transform( col_data )  #:, np.newaxis\n",
    "    print('scaled', data)\n",
    "    data = np.reshape(data, (1, len(data)))\n",
    "    df_scaled[cols[i]] = data[0]\n",
    "    \n",
    "\n",
    "# for data in datas:\n",
    "#     data_scaled = np.concatenate((data_scaled, data), axis=1)\n",
    "\n",
    "# scaler = MinMaxScaler() \n",
    "# data_scaled = scaler.fit_transform(data_array)\n",
    "# df_scaled = pd.DataFrame(data_scaled, columns=cols)\n",
    "\n",
    "display(df_scaled.head())\n",
    "\n",
    "# results in 0s ?? \n",
    "# scaler_y = MinMaxScaler()\n",
    "# scaler_y.fit_transform( [np.float32( np.diff(np.log(df['Close'] )  , axis=0)  )])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# cols = ['Timestamp','Volume_BTC', 'Bitcoin_Adj', 'Close']\n",
    "\n",
    "# for col in cols:\n",
    "#     df[col] = scaler.fit_transform([df[col].values])\n",
    "\n",
    "# X = df[['Timestamp', 'Volume_BTC', 'Bitcoin_Adj']].values\n",
    "# Y = df['Close'].values\n",
    "\n",
    "\n",
    "# X_train, X_test = X[1:train_size], X[train_size:len(X)]\n",
    "# Y_train, Y_test = Y[1:train_size], Y[train_size:len(X)]\n",
    "# print('Observations: %d' % (len(X)))\n",
    "# print('Training Observations: %d' % (len(X_train)))\n",
    "# print('Testing Observations: %d' % (len(Y_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split and reshape data to feed into RNN\n",
    "\n",
    "X_timestamp = df['Timestamp'].values\n",
    "X_volume = df['Volume_BTC'].values\n",
    "X_trends = df['Bitcoin_Adj'].values\n",
    "X_lagged_price = df['Price_lagged'].values\n",
    "\n",
    "Y_price = df['Close'].values\n",
    "\n",
    "train_size = int(len(X_timestamp) * 0.85)\n",
    "train_size = int(train_size/10) * 10 \n",
    "\n",
    "test_size_index = int(len(X_timestamp)/10)*10\n",
    "\n",
    "X_train_timestamp, X_test_timestamp = X_timestamp[:train_size], X_timestamp[train_size:test_size_index ]\n",
    "X_train_volume, X_test_volume = X_volume[:train_size], X_volume[train_size:test_size_index ]\n",
    "X_train_trends, X_test_trends = X_trends[:train_size], X_trends[train_size:test_size_index ]\n",
    "X_train_lagged_price, X_test_lagged_price = X_lagged_price[:train_size], X_lagged_price[train_size:test_size_index ]\n",
    "\n",
    "Y_train_price, Y_test_price = Y_price[:train_size], Y_price[train_size:test_size_index ]\n",
    "\n",
    "\n",
    "# X.shape is (samples, timesteps, dimension) \n",
    "# timestemps is 15, samples is just however many nobs there are (but it doesn't matter, so it should be None)\n",
    "\n",
    "\n",
    "\n",
    "X_train_timestamp = np.reshape(X_train_timestamp, (int(X_train_timestamp.shape[0]/data_length),data_length,1) ) \n",
    "X_train_volume = np.reshape(X_train_volume, (int(X_train_volume.shape[0]/data_length),data_length,1) ) \n",
    "X_train_trends = np.reshape(X_train_trends, (int(X_train_trends.shape[0]/data_length),data_length,1) ) \n",
    "X_train_lagged_price = np.reshape(X_train_lagged_price, (int(X_train_lagged_price.shape[0]/data_length), data_length, 1))\n",
    "\n",
    "X_test_timestamp = np.reshape(X_test_timestamp, (int(X_test_timestamp.shape[0]/data_length),data_length,1) ) \n",
    "X_test_volume = np.reshape(X_test_volume, (int(X_test_volume.shape[0]/data_length),data_length,1) ) \n",
    "X_test_trends = np.reshape(X_test_trends, (int(X_test_trends.shape[0]/data_length),data_length,1) )  \n",
    "X_test_lagged_price = np.reshape(X_test_lagged_price, (int(X_test_lagged_price.shape[0]/data_length),data_length,1))\n",
    "\n",
    "\n",
    "# Don't need the 1 for the third dimension for Y's??\n",
    "\n",
    "\n",
    "Y_train_price = np.reshape(Y_train_price, (int(Y_train_price.shape[0]/data_length),  data_length) ) \n",
    "Y_test_price = np.reshape(Y_test_price, (int(Y_test_price.shape[0]/data_length),  data_length) ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2386 samples, validate on 421 samples\n",
      "Epoch 1/250\n",
      "2386/2386 [==============================] - 4s - loss: 1798673.6938 - val_loss: 112369091.1544\n",
      "Epoch 2/250\n",
      "2386/2386 [==============================] - 3s - loss: 1782321.9708 - val_loss: 112204000.0570\n",
      "Epoch 3/250\n",
      "2386/2386 [==============================] - 3s - loss: 1767839.8560 - val_loss: 112040342.5178\n",
      "Epoch 4/250\n",
      "2386/2386 [==============================] - 3s - loss: 1753645.4811 - val_loss: 111876359.8860\n",
      "Epoch 5/250\n",
      "2386/2386 [==============================] - 3s - loss: 1739497.5148 - val_loss: 111713353.3682\n",
      "Epoch 6/250\n",
      "2386/2386 [==============================] - 3s - loss: 1725261.2288 - val_loss: 111548663.9050\n",
      "Epoch 7/250\n",
      "2386/2386 [==============================] - 3s - loss: 1711502.7217 - val_loss: 111386880.7791\n",
      "Epoch 8/250\n",
      "2386/2386 [==============================] - 3s - loss: 1697710.1849 - val_loss: 111223556.7506\n",
      "Epoch 9/250\n",
      "2386/2386 [==============================] - 3s - loss: 1684211.0629 - val_loss: 111062303.4299\n",
      "Epoch 10/250\n",
      "2386/2386 [==============================] - 3s - loss: 1670720.9914 - val_loss: 110900960.2660\n",
      "Epoch 11/250\n",
      "2386/2386 [==============================] - 3s - loss: 1657438.9450 - val_loss: 110738083.9905\n",
      "Epoch 12/250\n",
      "2386/2386 [==============================] - 3s - loss: 1644367.6055 - val_loss: 110577832.5701\n",
      "Epoch 13/250\n",
      "2386/2386 [==============================] - 3s - loss: 1631321.8225 - val_loss: 110414638.0998\n",
      "Epoch 14/250\n",
      "2386/2386 [==============================] - 3s - loss: 1618446.7833 - val_loss: 110253731.6865\n",
      "Epoch 15/250\n",
      "2386/2386 [==============================] - 3s - loss: 1605672.6210 - val_loss: 110093726.0618\n",
      "Epoch 16/250\n",
      "2386/2386 [==============================] - 3s - loss: 1593148.7994 - val_loss: 109935360.5701\n",
      "Epoch 17/250\n",
      "2386/2386 [==============================] - 3s - loss: 1580501.5419 - val_loss: 109770393.7292\n",
      "Epoch 18/250\n",
      "2386/2386 [==============================] - 3s - loss: 1568303.9273 - val_loss: 109611665.1781\n",
      "Epoch 19/250\n",
      "2386/2386 [==============================] - 3s - loss: 1556235.5959 - val_loss: 109453158.4988\n",
      "Epoch 20/250\n",
      "2386/2386 [==============================] - 3s - loss: 1544123.8024 - val_loss: 109290193.8242\n",
      "Epoch 21/250\n",
      "2386/2386 [==============================] - 3s - loss: 1532119.9851 - val_loss: 109131133.4347\n",
      "Epoch 22/250\n",
      "2386/2386 [==============================] - 3s - loss: 1520555.6769 - val_loss: 108973040.1900\n",
      "Epoch 23/250\n",
      "2386/2386 [==============================] - 3s - loss: 1508918.5608 - val_loss: 108815192.627168\n",
      "Epoch 24/250\n",
      "2386/2386 [==============================] - 3s - loss: 1497632.3821 - val_loss: 108658113.5392\n",
      "Epoch 25/250\n",
      "2386/2386 [==============================] - 3s - loss: 1486087.7056 - val_loss: 108496605.6057\n",
      "Epoch 26/250\n",
      "2386/2386 [==============================] - 3s - loss: 1474837.3334 - val_loss: 108337887.8670\n",
      "Epoch 27/250\n",
      "2386/2386 [==============================] - 3s - loss: 1463790.0012 - val_loss: 108179880.7791\n",
      "Epoch 28/250\n",
      "2386/2386 [==============================] - 3s - loss: 1452877.7921 - val_loss: 108021077.1306\n",
      "Epoch 29/250\n",
      "2386/2386 [==============================] - 3s - loss: 1441904.4181 - val_loss: 107862349.2637\n",
      "Epoch 30/250\n",
      "2386/2386 [==============================] - 3s - loss: 1431390.0912 - val_loss: 107703561.2732\n",
      "Epoch 31/250\n",
      "2386/2386 [==============================] - 3s - loss: 1420767.0721 - val_loss: 107548556.8076\n",
      "Epoch 32/250\n",
      "2386/2386 [==============================] - 3s - loss: 1410382.8472 - val_loss: 107390330.3753\n",
      "Epoch 33/250\n",
      "2386/2386 [==============================] - 4s - loss: 1400197.0068 - val_loss: 107236007.9430\n",
      "Epoch 34/250\n",
      "2386/2386 [==============================] - 3s - loss: 1390029.6951 - val_loss: 107076984.6651\n",
      "Epoch 35/250\n",
      "2386/2386 [==============================] - 3s - loss: 1380041.7551 - val_loss: 106920293.3587\n",
      "Epoch 36/250\n",
      "2386/2386 [==============================] - 3s - loss: 1370212.8308 - val_loss: 106763306.9644\n",
      "Epoch 37/250\n",
      "2386/2386 [==============================] - 3s - loss: 1360592.0181 - val_loss: 106608061.3587\n",
      "Epoch 38/250\n",
      "2386/2386 [==============================] - 3s - loss: 1350867.8973 - val_loss: 106450855.8670\n",
      "Epoch 39/250\n",
      "2386/2386 [==============================] - 3s - loss: 1341570.4827 - val_loss: 106299412.9216\n",
      "Epoch 40/250\n",
      "2386/2386 [==============================] - 3s - loss: 1332271.6634 - val_loss: 106142613.0166\n",
      "Epoch 41/250\n",
      "2386/2386 [==============================] - 3s - loss: 1323197.5245 - val_loss: 105988974.3848\n",
      "Epoch 42/250\n",
      "2386/2386 [==============================] - 3s - loss: 1314193.4863 - val_loss: 105835070.7933\n",
      "Epoch 43/250\n",
      "2386/2386 [==============================] - 3s - loss: 1305430.0959 - val_loss: 105681785.9002\n",
      "Epoch 44/250\n",
      "2386/2386 [==============================] - 3s - loss: 1296723.3821 - val_loss: 105526870.2613\n",
      "Epoch 45/250\n",
      "2386/2386 [==============================] - 3s - loss: 1288034.9768 - val_loss: 105372214.4323\n",
      "Epoch 46/250\n",
      "2386/2386 [==============================] - 3s - loss: 1279498.4455 - val_loss: 105218225.6532\n",
      "Epoch 47/250\n",
      "2386/2386 [==============================] - 3s - loss: 1271273.4000 - val_loss: 105066817.1211\n",
      "Epoch 48/250\n",
      "2386/2386 [==============================] - 3s - loss: 1263100.1254 - val_loss: 104916995.4014\n",
      "Epoch 49/250\n",
      "2386/2386 [==============================] - 3s - loss: 1255119.1574 - val_loss: 104765177.2257\n",
      "Epoch 50/250\n",
      "2386/2386 [==============================] - 3s - loss: 1247274.1449 - val_loss: 104614214.4703\n",
      "Epoch 51/250\n",
      "2386/2386 [==============================] - 3s - loss: 1239642.0947 - val_loss: 104462539.7720\n",
      "Epoch 52/250\n",
      "2386/2386 [==============================] - 3s - loss: 1231845.9850 - val_loss: 104311041.5867\n",
      "Epoch 53/250\n",
      "2386/2386 [==============================] - 3s - loss: 1224456.8552 - val_loss: 104162106.1283\n",
      "Epoch 54/250\n",
      "2386/2386 [==============================] - 3s - loss: 1217061.5138 - val_loss: 104012654.9929\n",
      "Epoch 55/250\n",
      "2386/2386 [==============================] - 3s - loss: 1209966.5214 - val_loss: 103864030.1758\n",
      "Epoch 56/250\n",
      "2386/2386 [==============================] - 3s - loss: 1202949.3202 - val_loss: 103715697.3302\n",
      "Epoch 57/250\n",
      "2386/2386 [==============================] - 3s - loss: 1196106.9089 - val_loss: 103570712.1615\n",
      "Epoch 58/250\n",
      "2386/2386 [==============================] - 3s - loss: 1189367.7557 - val_loss: 103420663.7340\n",
      "Epoch 59/250\n",
      "2386/2386 [==============================] - 3s - loss: 1182487.7606 - val_loss: 103274110.1948\n",
      "Epoch 60/250\n",
      "2386/2386 [==============================] - 3s - loss: 1176095.7196 - val_loss: 103126777.0071\n",
      "Epoch 61/250\n",
      "2386/2386 [==============================] - 3s - loss: 1169958.7686 - val_loss: 102988601.3777\n",
      "Epoch 62/250\n",
      "2386/2386 [==============================] - 3s - loss: 1163867.0334 - val_loss: 102843663.4869\n",
      "Epoch 63/250\n",
      "2386/2386 [==============================] - 3s - loss: 1157706.7001 - val_loss: 102697684.9976\n",
      "Epoch 64/250\n",
      "2386/2386 [==============================] - 3s - loss: 1151981.3919 - val_loss: 102556486.0713\n",
      "Epoch 65/250\n",
      "2386/2386 [==============================] - 3s - loss: 1146102.2586 - val_loss: 102415888.2090\n",
      "Epoch 66/250\n",
      "2386/2386 [==============================] - 3s - loss: 1140534.3036 - val_loss: 102273829.8622\n",
      "Epoch 67/250\n",
      "2386/2386 [==============================] - 3s - loss: 1135038.1831 - val_loss: 102133271.3254\n",
      "Epoch 68/250\n",
      "2386/2386 [==============================] - 3s - loss: 1129767.6757 - val_loss: 101992933.2542\n",
      "Epoch 69/250\n",
      "2386/2386 [==============================] - 3s - loss: 1124367.2408 - val_loss: 101850648.8646\n",
      "Epoch 70/250\n",
      "2386/2386 [==============================] - 3s - loss: 1119320.3533 - val_loss: 101712127.9050\n",
      "Epoch 71/250\n",
      "2386/2386 [==============================] - 3s - loss: 1114268.8806 - val_loss: 101576624.2090\n",
      "Epoch 72/250\n",
      "2386/2386 [==============================] - 3s - loss: 1109258.7393 - val_loss: 101432044.7601\n",
      "Epoch 73/250\n",
      "2386/2386 [==============================] - 3s - loss: 1104702.3513 - val_loss: 101303378.7933\n",
      "Epoch 74/250\n",
      "2386/2386 [==============================] - 3s - loss: 1100003.4282 - val_loss: 101161324.4846\n",
      "Epoch 75/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2386/2386 [==============================] - 3s - loss: 1095539.9481 - val_loss: 101023875.8955\n",
      "Epoch 76/250\n",
      "2386/2386 [==============================] - 3s - loss: 1091252.5698 - val_loss: 100891973.0356\n",
      "Epoch 77/250\n",
      "2386/2386 [==============================] - 3s - loss: 1086920.1774 - val_loss: 100752183.7815\n",
      "Epoch 78/250\n",
      "2386/2386 [==============================] - 3s - loss: 1082679.3911 - val_loss: 100620000.1520\n",
      "Epoch 79/250\n",
      "2386/2386 [==============================] - 3s - loss: 1078883.7381 - val_loss: 100492409.8432\n",
      "Epoch 80/250\n",
      "2386/2386 [==============================] - 3s - loss: 1075051.9191 - val_loss: 100364005.7577\n",
      "Epoch 81/250\n",
      "2386/2386 [==============================] - 3s - loss: 1071497.6967 - val_loss: 100238010.5083\n",
      "Epoch 82/250\n",
      "2386/2386 [==============================] - 3s - loss: 1067971.0942 - val_loss: 100107719.9810\n",
      "Epoch 83/250\n",
      "2386/2386 [==============================] - 3s - loss: 1064484.8411 - val_loss: 99985839.1924\n",
      "Epoch 84/250\n",
      "2386/2386 [==============================] - 3s - loss: 1061197.2460 - val_loss: 99857496.8361\n",
      "Epoch 85/250\n",
      "2386/2386 [==============================] - 3s - loss: 1058003.7310 - val_loss: 99733381.0166\n",
      "Epoch 86/250\n",
      "2386/2386 [==============================] - 3s - loss: 1054996.4698 - val_loss: 99613569.3682\n",
      "Epoch 87/250\n",
      "2386/2386 [==============================] - 3s - loss: 1051838.4955 - val_loss: 99494016.9311\n",
      "Epoch 88/250\n",
      "2386/2386 [==============================] - 3s - loss: 1049267.4125 - val_loss: 99380109.5392\n",
      "Epoch 89/250\n",
      "2386/2386 [==============================] - 3s - loss: 1046345.7096 - val_loss: 99257150.6318\n",
      "Epoch 90/250\n",
      "2386/2386 [==============================] - 3s - loss: 1043746.1444 - val_loss: 99137423.5819\n",
      "Epoch 91/250\n",
      "2386/2386 [==============================] - 3s - loss: 1041260.2391 - val_loss: 99023970.9549\n",
      "Epoch 92/250\n",
      "2386/2386 [==============================] - 3s - loss: 1038838.0366 - val_loss: 98905060.0380\n",
      "Epoch 93/250\n",
      "2386/2386 [==============================] - 3s - loss: 1036455.8237 - val_loss: 98791185.9192\n",
      "Epoch 94/250\n",
      "2386/2386 [==============================] - 3s - loss: 1034229.2890 - val_loss: 98681025.5202\n",
      "Epoch 95/250\n",
      "2386/2386 [==============================] - 3s - loss: 1032097.9392 - val_loss: 98565367.5629\n",
      "Epoch 96/250\n",
      "2386/2386 [==============================] - 3s - loss: 1029960.3749 - val_loss: 98454083.4869\n",
      "Epoch 97/250\n",
      "2386/2386 [==============================] - 4s - loss: 1028021.7926 - val_loss: 98348276.1425\n",
      "Epoch 98/250\n",
      "2386/2386 [==============================] - 3s - loss: 1026277.7476 - val_loss: 98247073.7387\n",
      "Epoch 99/250\n",
      "2386/2386 [==============================] - 3s - loss: 1024621.8911 - val_loss: 98148916.1235\n",
      "Epoch 100/250\n",
      "2386/2386 [==============================] - 3s - loss: 1023005.7016 - val_loss: 98048578.3468\n",
      "Epoch 101/250\n",
      "2386/2386 [==============================] - 3s - loss: 1021437.5765 - val_loss: 97948790.9549\n",
      "Epoch 102/250\n",
      "2386/2386 [==============================] - 3s - loss: 1020069.9511 - val_loss: 97860431.7150\n",
      "Epoch 103/250\n",
      "2386/2386 [==============================] - 3s - loss: 1018720.6062 - val_loss: 97763598.2043\n",
      "Epoch 104/250\n",
      "2386/2386 [==============================] - 3s - loss: 1017502.6387 - val_loss: 97680998.6793\n",
      "Epoch 105/250\n",
      "2386/2386 [==============================] - 3s - loss: 1016390.5707 - val_loss: 97593663.3159\n",
      "Epoch 106/250\n",
      "2386/2386 [==============================] - 3s - loss: 1015301.8933 - val_loss: 97512153.9192\n",
      "Epoch 107/250\n",
      "2386/2386 [==============================] - 3s - loss: 1014319.6920 - val_loss: 97434141.4917\n",
      "Epoch 108/250\n",
      "2386/2386 [==============================] - 3s - loss: 1013414.1623 - val_loss: 97356849.0261\n",
      "Epoch 109/250\n",
      "2386/2386 [==============================] - 3s - loss: 1012500.5994 - val_loss: 97281203.7530\n",
      "Epoch 110/250\n",
      "2386/2386 [==============================] - 3s - loss: 1011709.9671 - val_loss: 97209892.9596\n",
      "Epoch 111/250\n",
      "2386/2386 [==============================] - 3s - loss: 1010975.1574 - val_loss: 97145881.5962\n",
      "Epoch 112/250\n",
      "2386/2386 [==============================] - 3s - loss: 1010320.3748 - val_loss: 97075383.5059\n",
      "Epoch 113/250\n",
      "2386/2386 [==============================] - 3s - loss: 1009664.1873 - val_loss: 97015746.5653\n",
      "Epoch 114/250\n",
      "2386/2386 [==============================] - 3s - loss: 1009066.4762 - val_loss: 96952051.9905\n",
      "Epoch 115/250\n",
      "2386/2386 [==============================] - 3s - loss: 1008551.1838 - val_loss: 96898837.9762\n",
      "Epoch 116/250\n",
      "2386/2386 [==============================] - 3s - loss: 1008057.1355 - val_loss: 96834120.2280\n",
      "Epoch 117/250\n",
      "2386/2386 [==============================] - 3s - loss: 1007489.2217 - val_loss: 96772655.4489\n",
      "Epoch 118/250\n",
      "2386/2386 [==============================] - 3s - loss: 1007044.5148 - val_loss: 96716983.4964\n",
      "Epoch 119/250\n",
      "2386/2386 [==============================] - 3s - loss: 1006626.4230 - val_loss: 96661864.9026\n",
      "Epoch 120/250\n",
      "2386/2386 [==============================] - 3s - loss: 1006240.8055 - val_loss: 96616802.8219\n",
      "Epoch 121/250\n",
      "2386/2386 [==============================] - 3s - loss: 1005922.2550 - val_loss: 96572823.8860\n",
      "Epoch 122/250\n",
      "2386/2386 [==============================] - 3s - loss: 1005604.1794 - val_loss: 96519266.6793\n",
      "Epoch 123/250\n",
      "2386/2386 [==============================] - 3s - loss: 1005306.3233 - val_loss: 96479454.4228\n",
      "Epoch 124/250\n",
      "2386/2386 [==============================] - 3s - loss: 1005051.8204 - val_loss: 96441664.5986\n",
      "Epoch 125/250\n",
      "2386/2386 [==============================] - 3s - loss: 1004816.1252 - val_loss: 96400099.9430\n",
      "Epoch 126/250\n",
      "2386/2386 [==============================] - 3s - loss: 1004554.4221 - val_loss: 96358767.5724\n",
      "Epoch 127/250\n",
      "2386/2386 [==============================] - 3s - loss: 1004319.5103 - val_loss: 96313954.7458\n",
      "Epoch 128/250\n",
      "2386/2386 [==============================] - 3s - loss: 1004128.9423 - val_loss: 96281913.5962\n",
      "Epoch 129/250\n",
      "2386/2386 [==============================] - 3s - loss: 1003957.5938 - val_loss: 96249737.5677\n",
      "Epoch 130/250\n",
      "2386/2386 [==============================] - 3s - loss: 1003799.3852 - val_loss: 96218075.3539\n",
      "Epoch 131/250\n",
      "2386/2386 [==============================] - 3s - loss: 1003648.7363 - val_loss: 96184773.7862\n",
      "Epoch 132/250\n",
      "2386/2386 [==============================] - 3s - loss: 1003491.8928 - val_loss: 96147012.4846\n",
      "Epoch 133/250\n",
      "2386/2386 [==============================] - 3s - loss: 1003338.6207 - val_loss: 96114115.8575\n",
      "Epoch 134/250\n",
      "2386/2386 [==============================] - 3s - loss: 1003206.4912 - val_loss: 96083092.4466\n",
      "Epoch 135/250\n",
      "2386/2386 [==============================] - 3s - loss: 1003094.1998 - val_loss: 96056032.9881\n",
      "Epoch 136/250\n",
      "2386/2386 [==============================] - 3s - loss: 1003001.7196 - val_loss: 96033756.2090\n",
      "Epoch 137/250\n",
      "2386/2386 [==============================] - 3s - loss: 1002906.0815 - val_loss: 96004744.7506\n",
      "Epoch 138/250\n",
      "2386/2386 [==============================] - 3s - loss: 1002812.1130 - val_loss: 95976633.0831\n",
      "Epoch 139/250\n",
      "2386/2386 [==============================] - 3s - loss: 1002732.8126 - val_loss: 95956557.9002\n",
      "Epoch 140/250\n",
      "2386/2386 [==============================] - 3s - loss: 1002661.0097 - val_loss: 95935317.3492\n",
      "Epoch 141/250\n",
      "2386/2386 [==============================] - 3s - loss: 1002597.1370 - val_loss: 95912891.3824\n",
      "Epoch 142/250\n",
      "2386/2386 [==============================] - 3s - loss: 1002543.8560 - val_loss: 95893079.6295\n",
      "Epoch 143/250\n",
      "2386/2386 [==============================] - 3s - loss: 1002489.6597 - val_loss: 95876975.5819\n",
      "Epoch 144/250\n",
      "2386/2386 [==============================] - 3s - loss: 1002438.7440 - val_loss: 95855642.8979\n",
      "Epoch 145/250\n",
      "2386/2386 [==============================] - 3s - loss: 1002385.7009 - val_loss: 95836957.7862\n",
      "Epoch 146/250\n",
      "2386/2386 [==============================] - 3s - loss: 1002355.0404 - val_loss: 95825700.4086\n",
      "Epoch 147/250\n",
      "2386/2386 [==============================] - 3s - loss: 1002316.6852 - val_loss: 95807941.6437\n",
      "Epoch 148/250\n",
      "2386/2386 [==============================] - 3s - loss: 1002277.3612 - val_loss: 95793674.5843\n",
      "Epoch 149/250\n",
      "2386/2386 [==============================] - 3s - loss: 1002251.4033 - val_loss: 95781193.4347\n",
      "Epoch 150/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2386/2386 [==============================] - 3s - loss: 1002219.6845 - val_loss: 95765809.9572\n",
      "Epoch 151/250\n",
      "2386/2386 [==============================] - 3s - loss: 1002183.0772 - val_loss: 95747165.9382\n",
      "Epoch 152/250\n",
      "2386/2386 [==============================] - 3s - loss: 1002154.2198 - val_loss: 95736556.8646\n",
      "Epoch 153/250\n",
      "2386/2386 [==============================] - 3s - loss: 1002143.6370 - val_loss: 95726337.0831\n",
      "Epoch 154/250\n",
      "2386/2386 [==============================] - 3s - loss: 1002119.9334 - val_loss: 95714718.0903\n",
      "Epoch 155/250\n",
      "2386/2386 [==============================] - 3s - loss: 1002110.9405 - val_loss: 95706517.5772\n",
      "Epoch 156/250\n",
      "2386/2386 [==============================] - 3s - loss: 1002095.0031 - val_loss: 95699898.5083\n",
      "Epoch 157/250\n",
      "2386/2386 [==============================] - 3s - loss: 1002072.5185 - val_loss: 95689487.2874\n",
      "Epoch 158/250\n",
      "2386/2386 [==============================] - 3s - loss: 1002065.2206 - val_loss: 95682711.1829\n",
      "Epoch 159/250\n",
      "2386/2386 [==============================] - 3s - loss: 1002047.5347 - val_loss: 95672652.1805\n",
      "Epoch 160/250\n",
      "2386/2386 [==============================] - 3s - loss: 1002032.2194 - val_loss: 95661641.5107\n",
      "Epoch 161/250\n",
      "2386/2386 [==============================] - 3s - loss: 1002021.0087 - val_loss: 95654700.9311\n",
      "Epoch 162/250\n",
      "2386/2386 [==============================] - 3s - loss: 1002010.5830 - val_loss: 95645215.6770\n",
      "Epoch 163/250\n",
      "2386/2386 [==============================] - 3s - loss: 1002008.1513 - val_loss: 95641018.2803\n",
      "Epoch 164/250\n",
      "2386/2386 [==============================] - 3s - loss: 1001997.0916 - val_loss: 95633104.6651\n",
      "Epoch 165/250\n",
      "2386/2386 [==============================] - 3s - loss: 1001990.2963 - val_loss: 95626618.6983\n",
      "Epoch 166/250\n",
      "2386/2386 [==============================] - 3s - loss: 1001979.7125 - val_loss: 95623113.6722\n",
      "Epoch 167/250\n",
      "2386/2386 [==============================] - 3s - loss: 1001977.6475 - val_loss: 95618143.1639\n",
      "Epoch 168/250\n",
      "2386/2386 [==============================] - 3s - loss: 1001968.1543 - val_loss: 95612211.3729\n",
      "Epoch 169/250\n",
      "2386/2386 [==============================] - 3s - loss: 1001960.8218 - val_loss: 95601730.3658\n",
      "Epoch 170/250\n",
      "2386/2386 [==============================] - 3s - loss: 1001948.7755 - val_loss: 95595656.9596\n",
      "Epoch 171/250\n",
      "2386/2386 [==============================] - 3s - loss: 1001946.6530 - val_loss: 95587828.8266\n",
      "Epoch 172/250\n",
      "2386/2386 [==============================] - 3s - loss: 1001932.1055 - val_loss: 95583031.8100\n",
      "Epoch 173/250\n",
      "2386/2386 [==============================] - 3s - loss: 1001937.7438 - val_loss: 95576549.4157\n",
      "Epoch 174/250\n",
      "2386/2386 [==============================] - 3s - loss: 1001924.3457 - val_loss: 95571012.8076\n",
      "Epoch 175/250\n",
      "2386/2386 [==============================] - 3s - loss: 1001924.0789 - val_loss: 95563221.2637\n",
      "Epoch 176/250\n",
      "2386/2386 [==============================] - 3s - loss: 1001916.5736 - val_loss: 95557268.5796\n",
      "Epoch 177/250\n",
      "2386/2386 [==============================] - 3s - loss: 1001910.5977 - val_loss: 95552633.6342\n",
      "Epoch 178/250\n",
      "2386/2386 [==============================] - 3s - loss: 1001907.4875 - val_loss: 95547508.4466\n",
      "Epoch 179/250\n",
      "2386/2386 [==============================] - 3s - loss: 1001907.1301 - val_loss: 95542444.4086\n",
      "Epoch 180/250\n",
      "2386/2386 [==============================] - 3s - loss: 1001904.7315 - val_loss: 95539071.1734\n",
      "Epoch 181/250\n",
      "2386/2386 [==============================] - 3s - loss: 1001897.1736 - val_loss: 95535748.0190\n",
      "Epoch 182/250\n",
      "2386/2386 [==============================] - 3s - loss: 1001902.6337 - val_loss: 95533516.8456\n",
      "Epoch 183/250\n",
      "2386/2386 [==============================] - 3s - loss: 1001898.0003 - val_loss: 95531163.6770\n",
      "Epoch 184/250\n",
      "2386/2386 [==============================] - 3s - loss: 1001896.5591 - val_loss: 95528091.3444\n",
      "Epoch 185/250\n",
      "2386/2386 [==============================] - 3s - loss: 1001890.7382 - val_loss: 95524442.62239418.\n",
      "Epoch 186/250\n",
      "2386/2386 [==============================] - 3s - loss: 1001889.7778 - val_loss: 95520685.6152\n",
      "Epoch 187/250\n",
      "2386/2386 [==============================] - 3s - loss: 1001888.7991 - val_loss: 95515509.0546\n",
      "Epoch 188/250\n",
      "2386/2386 [==============================] - 3s - loss: 1001894.4271 - val_loss: 95516215.6485\n",
      "Epoch 189/250\n",
      "2386/2386 [==============================] - 3s - loss: 1001882.9814 - val_loss: 95512824.5416\n",
      "Epoch 190/250\n",
      "2386/2386 [==============================] - 3s - loss: 1001884.3354 - val_loss: 95511783.4394\n",
      "Epoch 191/250\n",
      "2386/2386 [==============================] - 3s - loss: 1001889.2279 - val_loss: 95510340.3990\n",
      "Epoch 192/250\n",
      "2386/2386 [==============================] - 3s - loss: 1001887.0160 - val_loss: 95503953.1781\n",
      "Epoch 193/250\n",
      "2386/2386 [==============================] - 3s - loss: 1001880.9453 - val_loss: 95502726.2328\n",
      "Epoch 194/250\n",
      "2386/2386 [==============================] - 3s - loss: 1001879.7009 - val_loss: 95501296.4371\n",
      "Epoch 195/250\n",
      "2386/2386 [==============================] - 3s - loss: 1001880.5058 - val_loss: 95501096.0570\n",
      "Epoch 196/250\n",
      "2386/2386 [==============================] - 3s - loss: 1001883.6151 - val_loss: 95500756.6176\n",
      "Epoch 197/250\n",
      "2386/2386 [==============================] - 3s - loss: 1001880.7899 - val_loss: 95498758.4418\n",
      "Epoch 198/250\n",
      "2386/2386 [==============================] - 3s - loss: 1001877.9399 - val_loss: 95496733.7767\n",
      "Epoch 199/250\n",
      "2386/2386 [==============================] - 3s - loss: 1001877.5522 - val_loss: 95494084.0950\n",
      "Epoch 200/250\n",
      "2386/2386 [==============================] - 3s - loss: 1001884.3149 - val_loss: 95486924.5511\n",
      "Epoch 201/250\n",
      "2386/2386 [==============================] - 3s - loss: 1001870.5351 - val_loss: 95487215.6770\n",
      "Epoch 202/250\n",
      "2386/2386 [==============================] - 3s - loss: 1001878.6465 - val_loss: 95488848.8741\n",
      "Epoch 203/250\n",
      "2386/2386 [==============================] - 3s - loss: 1001878.0787 - val_loss: 95486175.0119\n",
      "Epoch 204/250\n",
      "2386/2386 [==============================] - 3s - loss: 1001871.1563 - val_loss: 95483145.5677\n",
      "Epoch 205/250\n",
      "2386/2386 [==============================] - 3s - loss: 1001876.5159 - val_loss: 95480145.6912\n",
      "Epoch 206/250\n",
      "2386/2386 [==============================] - 3s - loss: 1001872.2770 - val_loss: 95477480.0570\n",
      "Epoch 207/250\n",
      "2386/2386 [==============================] - 3s - loss: 1001871.8968 - val_loss: 95474225.6437\n",
      "Epoch 208/250\n",
      "2386/2386 [==============================] - 3s - loss: 1001868.4768 - val_loss: 95475143.6865\n",
      "Epoch 209/250\n",
      "2386/2386 [==============================] - 3s - loss: 1001874.4898 - val_loss: 95474246.6128\n",
      "Epoch 210/250\n",
      "2386/2386 [==============================] - 3s - loss: 1001873.9154 - val_loss: 95474644.2850\n",
      "Epoch 211/250\n",
      "2386/2386 [==============================] - 3s - loss: 1001868.0553 - val_loss: 95477200.8836\n",
      "Epoch 212/250\n",
      "2386/2386 [==============================] - 3s - loss: 1001873.0197 - val_loss: 95474958.3468\n",
      "Epoch 213/250\n",
      "2386/2386 [==============================] - 3s - loss: 1001871.7520 - val_loss: 95474919.2019\n",
      "Epoch 214/250\n",
      "2386/2386 [==============================] - 3s - loss: 1001879.9929 - val_loss: 95476103.3539\n",
      "Epoch 215/250\n",
      "2386/2386 [==============================] - 3s - loss: 1001872.7377 - val_loss: 95475654.0808\n",
      "Epoch 216/250\n",
      "2386/2386 [==============================] - 3s - loss: 1001868.4889 - val_loss: 95474892.0095\n",
      "Epoch 217/250\n",
      "2386/2386 [==============================] - 3s - loss: 1001870.9451 - val_loss: 95474728.9501\n",
      "Epoch 218/250\n",
      "2386/2386 [==============================] - 3s - loss: 1001872.5105 - val_loss: 95475580.2945\n",
      "Epoch 219/250\n",
      "2386/2386 [==============================] - 3s - loss: 1001870.7354 - val_loss: 95473205.2162\n",
      "Epoch 220/250\n",
      "2386/2386 [==============================] - 3s - loss: 1001880.7952 - val_loss: 95477395.9240\n",
      "Epoch 221/250\n",
      "2386/2386 [==============================] - 3s - loss: 1001879.2654 - val_loss: 95479605.4252\n",
      "Epoch 222/250\n",
      "2386/2386 [==============================] - 3s - loss: 1001872.0135 - val_loss: 95481265.5012\n",
      "Epoch 223/250\n",
      "2386/2386 [==============================] - 3s - loss: 1001871.8309 - val_loss: 95481158.9834\n",
      "Epoch 224/250\n",
      "2386/2386 [==============================] - 3s - loss: 1001873.0733 - val_loss: 95479283.0214\n",
      "Epoch 225/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2386/2386 [==============================] - 3s - loss: 1001872.4969 - val_loss: 95477374.1948\n",
      "Epoch 226/250\n",
      "2386/2386 [==============================] - 3s - loss: 1001871.8167 - val_loss: 95478602.4038\n",
      "Epoch 227/250\n",
      "2386/2386 [==============================] - 3s - loss: 1001871.5236 - val_loss: 95476478.9929\n",
      "Epoch 228/250\n",
      "2386/2386 [==============================] - 3s - loss: 1001874.9477 - val_loss: 95477078.5653\n",
      "Epoch 229/250\n",
      "2386/2386 [==============================] - 3s - loss: 1001857.9328 - val_loss: 95485319.5914\n",
      "Epoch 230/250\n",
      "2386/2386 [==============================] - 3s - loss: 1001875.4369 - val_loss: 95483554.4513\n",
      "Epoch 231/250\n",
      "2386/2386 [==============================] - 3s - loss: 1001868.8661 - val_loss: 95486515.9715\n",
      "Epoch 232/250\n",
      "2386/2386 [==============================] - 3s - loss: 1001875.3595 - val_loss: 95488440.4181\n",
      "Epoch 233/250\n",
      "2386/2386 [==============================] - 3s - loss: 1001877.3999 - val_loss: 95486464.6746\n",
      "Epoch 234/250\n",
      "2386/2386 [==============================] - 3s - loss: 1001873.8675 - val_loss: 95487186.1568\n",
      "Epoch 235/250\n",
      "2386/2386 [==============================] - 3s - loss: 1001875.9807 - val_loss: 95484420.6176\n",
      "Epoch 236/250\n",
      "2386/2386 [==============================] - 3s - loss: 1001877.8132 - val_loss: 95484771.7245\n",
      "Epoch 237/250\n",
      "2386/2386 [==============================] - 3s - loss: 1001880.3915 - val_loss: 95486402.9454\n",
      "Epoch 238/250\n",
      "2386/2386 [==============================] - 3s - loss: 1001872.4417 - val_loss: 95487645.0261\n",
      "Epoch 239/250\n",
      "2386/2386 [==============================] - 3s - loss: 1001872.4660 - val_loss: 95484576.6841\n",
      "Epoch 240/250\n",
      "2386/2386 [==============================] - 3s - loss: 1001882.2888 - val_loss: 95487024.5891\n",
      "Epoch 241/250\n",
      "2386/2386 [==============================] - 3s - loss: 1001874.3194 - val_loss: 95485334.4038\n",
      "Epoch 242/250\n",
      "2386/2386 [==============================] - 3s - loss: 1001877.5162 - val_loss: 95486684.7506\n",
      "Epoch 243/250\n",
      "2386/2386 [==============================] - 3s - loss: 1001879.8812 - val_loss: 95481343.6390\n",
      "Epoch 244/250\n",
      "2386/2386 [==============================] - 3s - loss: 1001870.9037 - val_loss: 95482258.8694\n",
      "Epoch 245/250\n",
      "2386/2386 [==============================] - 3s - loss: 1001874.1476 - val_loss: 95481075.7055\n",
      "Epoch 246/250\n",
      "2386/2386 [==============================] - 3s - loss: 1001871.2958 - val_loss: 95477383.0594\n",
      "Epoch 247/250\n",
      "2386/2386 [==============================] - 3s - loss: 1001873.0620 - val_loss: 95476492.9216\n",
      "Epoch 248/250\n",
      "2386/2386 [==============================] - 3s - loss: 1001868.9546 - val_loss: 95474642.6318\n",
      "Epoch 249/250\n",
      "2386/2386 [==============================] - 3s - loss: 1001871.8365 - val_loss: 95475831.8005\n",
      "Epoch 250/250\n",
      "2386/2386 [==============================] - 3s - loss: 1001872.4081 - val_loss: 95474934.8979\n"
     ]
    }
   ],
   "source": [
    "features = ['Volume_BTC', 'Bitcoin_Adj', 'Price_lagged']\n",
    "\n",
    "rnn = build_model(features, 10, 10) \n",
    "\n",
    "tensorboard_callback = TensorBoard(log_dir='./Graph', histogram_freq=0, write_graph=True, write_images=True)\n",
    "\n",
    "history = rnn.fit(\n",
    "    [\n",
    "        #X_train_timestamp,\n",
    "        X_train_volume,\n",
    "        X_train_trends,\n",
    "        X_train_lagged_price\n",
    "    ],\n",
    "    [\n",
    "        Y_train_price\n",
    "    ]\n",
    "    ,\n",
    "    validation_data=(\n",
    "        [\n",
    "            #X_test_timestamp,\n",
    "            X_test_volume,\n",
    "            X_test_trends,\n",
    "            X_test_lagged_price\n",
    "        ],\n",
    "        [\n",
    "            Y_test_price\n",
    "        ]),\n",
    "    epochs=250,\n",
    "    batch_size=32,\n",
    "    callbacks=[\n",
    "      tensorboard_callback\n",
    "    ],\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEDCAYAAADOc0QpAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x15c8fa30d30>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# plot history\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='test')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "421/421 [==============================] - 0s     \n",
      "95474934.8979\n"
     ]
    }
   ],
   "source": [
    "score = rnn.evaluate(\n",
    "    [\n",
    "        #X_test_timestamp,\n",
    "        X_test_volume,\n",
    "        X_test_trends,\n",
    "        X_test_lagged_price\n",
    "    ],\n",
    "    [\n",
    "        Y_test_price\n",
    "    ])\n",
    "\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "non-broadcastable output operand with shape (4210,1) doesn't match the broadcast shape (4210,4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-94-6a95922dc1eb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;31m# invert scaling\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0minv_yhat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0myhat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_size_index\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mtrain_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0minv_yhat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minverse_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minv_yhat\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# it forecasts the next 10 hours, so just reshape to 1D\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0mY_test_price\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mY_test_price\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_size_index\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mtrain_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\sklearn\\preprocessing\\data.py\u001b[0m in \u001b[0;36minverse_transform\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    383\u001b[0m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFLOAT_DTYPES\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    384\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 385\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m-=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmin_\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    386\u001b[0m         \u001b[0mX\u001b[0m \u001b[1;33m/=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscale_\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    387\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: non-broadcastable output operand with shape (4210,1) doesn't match the broadcast shape (4210,4)"
     ]
    }
   ],
   "source": [
    "yhat = rnn.predict( \n",
    "    [\n",
    "        #X_test_timestamp,\n",
    "        X_test_volume,\n",
    "        X_test_trends,\n",
    "        X_test_lagged_price\n",
    "    ],\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# invert scaling \n",
    "inv_yhat = yhat.reshape((test_size_index - train_size, 1 ))\n",
    "\n",
    "    # tack on other data to unscale \n",
    "\n",
    "\n",
    "inv_yhat = scaler.inverse_transform(inv_yhat) # it forecasts the next 10 hours, so just reshape to 1D  \n",
    "\n",
    "Y_test_price = Y_test_price.reshape((test_size_index - train_size, 1 ))\n",
    "Y_test_price = scaler_y.inverse_transform(inv_yhat) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# serialize model to JSON\n",
    "model_json = rnn.to_json()\n",
    "with open(\"model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "    \n",
    "# serialize weights to HDF5\n",
    "rnn.save_weights(\"model.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Probably need to change up x and y to frame it as a supervising problem -> look online to that one article\n",
    "# also scale it from 0 to 1 \n",
    "# remove seasonality \n",
    "# Change timestamp into some categorial input, not just a timestamp \n",
    "\n",
    "# look into if min-max scaling actually worked properly "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
