{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from keras import Input\n",
    "from keras.engine import Model\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers import Concatenate, concatenate\n",
    "from keras.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# features is a list of strings of feature names \n",
    "\n",
    "def build_model(features, data_length):\n",
    "    \n",
    "    inputs_list = [] \n",
    "    for feature_name in features:\n",
    "        inputs_list.append((Input(shape=(data_length,1), name=feature_name)))\n",
    "    \n",
    "    layers = [] \n",
    "    for i, input_name in enumerate(inputs_list): \n",
    "        layers.append(LSTM(64, return_sequences=False)(inputs_list[i]) )\n",
    "        \n",
    "    output = concatenate(layers) \n",
    "    output = Dense(3, activation='softmax', name='IsSpike')(output)\n",
    "    \n",
    "    model = Model(\n",
    "        inputs = inputs_list,\n",
    "        outputs = [output]\n",
    "    )\n",
    "    \n",
    "    model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "    \n",
    "    return model    \n",
    "\n",
    "data_length = 20\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Shape\n",
    "\n",
    "* Price  ----------> LSTM --\\\n",
    "* Google Trends ---> LSTM ---> Dense Layer -> Softmax -> Output: Is Spike (1,0,-1) \n",
    "* Volume ----------> LSTM --/\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "* Input: Price, Google Trends, and Volume for time t0-t9 (10 hours of data) \n",
    "* Output: Is Spike (1,0,-1) for t10 \n",
    "    * Using 10 hours (t0-t9) of Price, Google Trends, and Volume to predict the price movement at t11 (t10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Shoya\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\__main__.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\Shoya\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\__main__.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\Shoya\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\__main__.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Volume_BTC</th>\n",
       "      <th>Bitcoin_Adj</th>\n",
       "      <th>Close</th>\n",
       "      <th>Price_lagged</th>\n",
       "      <th>Is Spike</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.776791</td>\n",
       "      <td>0.471970</td>\n",
       "      <td>0.484557</td>\n",
       "      <td>0.484557</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.463316</td>\n",
       "      <td>0.439996</td>\n",
       "      <td>0.538331</td>\n",
       "      <td>0.538331</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.725079</td>\n",
       "      <td>0.529463</td>\n",
       "      <td>0.520715</td>\n",
       "      <td>0.520715</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.210661</td>\n",
       "      <td>0.416611</td>\n",
       "      <td>0.566098</td>\n",
       "      <td>0.566098</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.594148</td>\n",
       "      <td>0.445509</td>\n",
       "      <td>0.568881</td>\n",
       "      <td>0.568881</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Volume_BTC  Bitcoin_Adj     Close  Price_lagged  Is Spike\n",
       "1    0.776791     0.471970  0.484557      0.484557       1.0\n",
       "2    0.463316     0.439996  0.538331      0.538331       1.0\n",
       "3    0.725079     0.529463  0.520715      0.520715      -1.0\n",
       "4    0.210661     0.416611  0.566098      0.566098       0.0\n",
       "5    0.594148     0.445509  0.568881      0.568881      -1.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Volume_BTC</th>\n",
       "      <th>Bitcoin_Adj</th>\n",
       "      <th>Close</th>\n",
       "      <th>Price_lagged</th>\n",
       "      <th>Is Spike</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>30285</th>\n",
       "      <td>0.455905</td>\n",
       "      <td>0.449846</td>\n",
       "      <td>0.546519</td>\n",
       "      <td>0.546519</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30286</th>\n",
       "      <td>0.308749</td>\n",
       "      <td>0.457405</td>\n",
       "      <td>0.549938</td>\n",
       "      <td>0.549938</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30287</th>\n",
       "      <td>0.632915</td>\n",
       "      <td>0.453625</td>\n",
       "      <td>0.563428</td>\n",
       "      <td>0.563428</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30288</th>\n",
       "      <td>0.629822</td>\n",
       "      <td>0.434124</td>\n",
       "      <td>0.506657</td>\n",
       "      <td>0.506657</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30289</th>\n",
       "      <td>0.564642</td>\n",
       "      <td>0.445374</td>\n",
       "      <td>0.493535</td>\n",
       "      <td>0.493535</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Volume_BTC  Bitcoin_Adj     Close  Price_lagged  Is Spike\n",
       "30285    0.455905     0.449846  0.546519      0.546519       1.0\n",
       "30286    0.308749     0.457405  0.549938      0.549938      -1.0\n",
       "30287    0.632915     0.453625  0.563428      0.563428       0.0\n",
       "30288    0.629822     0.434124  0.506657      0.506657       0.0\n",
       "30289    0.564642     0.445374  0.493535      0.493535      -1.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "master_df = pd.read_csv('C:/Users/Shoya/surf/data/master_df_v3.csv', encoding='latin1')\n",
    "df = master_df[['Timestamp', 'Close', 'Volume_(BTC)', 'Volume_(Currency)', 'Date(UTC)', 'Bitcoin (Adj.Overlap)', \n",
    "               'Close Price % Change', 'Close Price % Change (Abs)', 'Is Spike']]\n",
    "\n",
    "# lag inputs depending on data_length \n",
    "df['Price_lagged'] = df['Close']#.shift(data_length)\n",
    "df['Volume_BTC'] = df['Volume_(BTC)']#.shift(data_length)\n",
    "df['Bitcoin_Adj'] = df['Bitcoin (Adj.Overlap)']#.shift(data_length)\n",
    "\n",
    "df = df.dropna()\n",
    "cols = ['Volume_BTC','Bitcoin_Adj', 'Close', 'Price_lagged']\n",
    "\n",
    "# Stationalize Data by taking log differences\n",
    "data_array = np.diff(np.log(df[cols]), axis=0)\n",
    "\n",
    "# Min-Max Scale \n",
    "\n",
    "scalers = {}\n",
    "datas = [] \n",
    "\n",
    "df_scaled = pd.DataFrame(columns=cols)\n",
    "\n",
    "\n",
    "############################################################\n",
    "#  Fix below - I am scaling the whole data set together, when I should scale the train and test datasets separately\n",
    "############################################################\n",
    "\n",
    "for i in range(len(cols)): \n",
    "    scalers[cols[i]] = MinMaxScaler()\n",
    "    #print('data', data_array[:,i])\n",
    "    \n",
    "    col_data = data_array[:,i]\n",
    "    col_data = np.reshape(col_data, (len(col_data), 1))\n",
    "    \n",
    "    data = scalers[cols[i]].fit_transform( col_data )\n",
    "    #print('scaled', data)\n",
    "    data = np.reshape(data, (1, len(data)))\n",
    "    df_scaled[cols[i]] = data[0]\n",
    "    \n",
    "df_scaled['Is Spike'] = df['Is Spike']\n",
    "df_scaled.dropna(inplace=True)\n",
    "display(df_scaled.head())\n",
    "display(df_scaled.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0., -1.,  1., ...,  1.,  1., -1.])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "# split and reshape data to feed into RNN\n",
    "\n",
    "# X_timestamp = df_scaled['Timestamp'].values\n",
    "X_volume = df_scaled['Volume_BTC'].values\n",
    "X_trends = df_scaled['Bitcoin_Adj'].values\n",
    "X_lagged_price = df_scaled['Price_lagged'].values\n",
    "\n",
    "Y_is_spike = df_scaled['Is Spike'].values \n",
    "\n",
    "train_size = int(len(X_volume) * 0.85)\n",
    "train_size = int(train_size/data_length) * data_length\n",
    "\n",
    "test_size_index = int(len(X_volume)/data_length)*data_length\n",
    "\n",
    "X_train_volume = []\n",
    "X_test_volume = [] \n",
    "X_train_trends = []\n",
    "X_test_trends = []\n",
    "X_train_lagged_price = []\n",
    "X_test_lagged_price = []\n",
    "Y_train_is_spike = [] \n",
    "Y_test_is_spike = [] \n",
    "\n",
    "for i in range(train_size-data_length):\n",
    "    vol_temp = []\n",
    "    trends_temp = []\n",
    "    price_temp = []\n",
    "    for j in range(data_length):\n",
    "        vol_temp.append(X_volume[i+j])\n",
    "        trends_temp.append(X_trends[i+j])\n",
    "        price_temp.append(X_lagged_price[i+j])\n",
    "    X_train_volume.append(vol_temp)\n",
    "    X_train_trends.append(trends_temp)\n",
    "    X_train_lagged_price.append(price_temp)\n",
    "    \n",
    "    Y_train_is_spike.append(Y_is_spike[i+data_length])\n",
    "\n",
    "for i in range(test_size_index-train_size-data_length):\n",
    "    vol_temp = []\n",
    "    trends_temp = [] \n",
    "    price_temp = [] \n",
    "    for j in range(data_length):\n",
    "        vol_temp.append(X_volume[train_size+i+j])\n",
    "        trends_temp.append(X_trends[train_size+i+j])\n",
    "        price_temp.append(X_lagged_price[train_size+i+j])\n",
    "    X_test_volume.append(vol_temp)\n",
    "    X_test_trends.append(trends_temp)\n",
    "    X_test_lagged_price.append(price_temp)\n",
    "    \n",
    "    Y_test_is_spike.append(Y_is_spike[train_size+i+data_length])\n",
    "    \n",
    "X_train_volume = np.array(X_train_volume)\n",
    "X_test_volume =  np.array(X_test_volume)\n",
    "X_train_trends = np.array(X_train_trends)\n",
    "X_test_trends = np.array(X_test_trends)\n",
    "X_train_lagged_price = np.array(X_train_lagged_price)\n",
    "X_test_lagged_price = np.array(X_test_lagged_price)\n",
    "Y_train_is_spike =  np.array(Y_train_is_spike)\n",
    "Y_test_is_spike = np.array(Y_test_is_spike)\n",
    "    \n",
    "    \n",
    "Y_train_is_spike_onehot = to_categorical(Y_train_is_spike, num_classes=3)\n",
    "Y_test_is_spike_onehot = to_categorical(Y_test_is_spike,num_classes=3)\n",
    "display(Y_train_is_spike)\n",
    "\n",
    "# y = pd.DataFrame(Y_train_is_spike_onehot)\n",
    "# y['actual'] = Y_train_is_spike\n",
    "# display(y.head(25))\n",
    "    \n",
    "# display(X_train_trends.shape)\n",
    "# display(Y_train_is_spike.shape)\n",
    "\n",
    "#display(X_train_lagged_price)\n",
    "#display(Y_train_is_spike)\n",
    "\n",
    "# df_train = pd.DataFrame(X_train_lagged_price)\n",
    "# df_train['label'] = Y_train_is_spike\n",
    "# display(df_train.tail(20))\n",
    "# display(df_scaled.head(30))\n",
    "# display(df_train.head(30))\n",
    "\n",
    "#--------------------------------\n",
    "\n",
    "# # X_train_timestamp, X_test_timestamp = X_timestamp[:train_size], X_timestamp[train_size:test_size_index ]\n",
    "# X_train_volume, X_test_volume = X_volume[:train_size], X_volume[train_size:test_size_index ]\n",
    "# X_train_trends, X_test_trends = X_trends[:train_size], X_trends[train_size:test_size_index ]\n",
    "# X_train_lagged_price, X_test_lagged_price = X_lagged_price[:train_size], X_lagged_price[train_size:test_size_index ]\n",
    "\n",
    "# # becasue I lagged the x inputs, I should forward the Y's by the data_length as well \n",
    "# Y_train_is_spike, Y_test_is_spike = Y_is_spike[data_length:train_size], Y_is_spike[train_size+data_length:test_size_index ]\n",
    "\n",
    "\n",
    "# # X.shape is (samples, timesteps, dimension) \n",
    "# # timestemps is 15, samples is just however many nobs there are (but it doesn't matter, so it should be None)\n",
    "\n",
    "\n",
    "X_train_volume = np.reshape(X_train_volume, (X_train_volume.shape[0],data_length,1) ) \n",
    "X_train_trends = np.reshape(X_train_trends, (X_train_trends.shape[0],data_length,1) ) \n",
    "X_train_lagged_price = np.reshape(X_train_lagged_price, (X_train_lagged_price.shape[0], data_length, 1))\n",
    "\n",
    "X_test_volume = np.reshape(X_test_volume, (X_test_volume.shape[0],data_length,1) ) \n",
    "X_test_trends = np.reshape(X_test_trends, (X_test_trends.shape[0],data_length,1) )  \n",
    "X_test_lagged_price = np.reshape(X_test_lagged_price, (X_test_lagged_price.shape[0],data_length,1))\n",
    "\n",
    "\n",
    "# # X_train_timestamp = np.reshape(X_train_timestamp, (int(X_train_timestamp.shape[0]/data_length),data_length,1) ) \n",
    "# X_train_volume = np.reshape(X_train_volume, (int(X_train_volume.shape[0]/data_length),data_length,1) ) \n",
    "# X_train_trends = np.reshape(X_train_trends, (int(X_train_trends.shape[0]/data_length),data_length,1) ) \n",
    "# X_train_lagged_price = np.reshape(X_train_lagged_price, (int(X_train_lagged_price.shape[0]/data_length), data_length, 1))\n",
    "\n",
    "# # X_test_timestamp = np.reshape(X_test_timestamp, (int(X_test_timestamp.shape[0]/data_length),data_length,1) ) \n",
    "# X_test_volume = np.reshape(X_test_volume, (int(X_test_volume.shape[0]/data_length),data_length,1) ) \n",
    "# X_test_trends = np.reshape(X_test_trends, (int(X_test_trends.shape[0]/data_length),data_length,1) )  \n",
    "# X_test_lagged_price = np.reshape(X_test_lagged_price, (int(X_test_lagged_price.shape[0]/data_length),data_length,1))\n",
    "\n",
    "\n",
    "# # Don't need the 1 for the third dimension for Y's??\n",
    "\n",
    "\n",
    "# Y_train_is_spike = np.reshape(Y_train_is_spike, (int(Y_train_is_spike.shape[0]/data_length),  data_length) ) \n",
    "# Y_test_is_spike = np.reshape(Y_test_is_spike, (int(Y_test_is_spike.shape[0]/data_length),  data_length) )\n",
    "\n",
    "#-----------------------------------\n",
    "\n",
    "\n",
    "# instead of using input 1,2,3,4,5,6,7,8,9,10 to predict output for 11,12,13,14,15,16,17,18,19,20\n",
    "# I want to use input 1,2,3,4,5,6,7,8,9,10 to predict output for 11, then 2,3,4,5,6,7,8,9,10,11 to predict output for 12 \n",
    "\n",
    "# right now I am actually feeding input 1,2,3,4,5,6,7,8,9,10 to predict output for 1,2,3,4,5,6,7,8,9,10. \n",
    "# instead I should at least feed 1,2,3..8,9,10 to predict 11,12,13,14,15,16,17,18,19,20 -> lag everything by data_length! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "25720/25720 [==============================] - 53s - loss: 1.0819 - categorical_accuracy: 0.4242    \n",
      "Epoch 2/100\n",
      "25720/25720 [==============================] - 51s - loss: 1.0814 - categorical_accuracy: 0.4242    \n",
      "Epoch 3/100\n",
      "25720/25720 [==============================] - 49s - loss: 1.0799 - categorical_accuracy: 0.4242    \n",
      "Epoch 4/100\n",
      "25720/25720 [==============================] - 51s - loss: 1.0759 - categorical_accuracy: 0.4262    \n",
      "Epoch 5/100\n",
      "25720/25720 [==============================] - 49s - loss: 1.0720 - categorical_accuracy: 0.4338    \n",
      "Epoch 6/100\n",
      "25720/25720 [==============================] - 49s - loss: 1.0673 - categorical_accuracy: 0.4459    \n",
      "Epoch 7/100\n",
      "25720/25720 [==============================] - 49s - loss: 1.0605 - categorical_accuracy: 0.4563    \n",
      "Epoch 8/100\n",
      "25720/25720 [==============================] - 48s - loss: 1.0546 - categorical_accuracy: 0.4678    \n",
      "Epoch 9/100\n",
      "25720/25720 [==============================] - 49s - loss: 1.0487 - categorical_accuracy: 0.4818    \n",
      "Epoch 10/100\n",
      "25720/25720 [==============================] - 49s - loss: 1.0428 - categorical_accuracy: 0.4945    \n",
      "Epoch 11/100\n",
      "25720/25720 [==============================] - 49s - loss: 1.0258 - categorical_accuracy: 0.5104    \n",
      "Epoch 12/100\n",
      "25720/25720 [==============================] - 49s - loss: 1.0046 - categorical_accuracy: 0.5191    \n",
      "Epoch 13/100\n",
      "25720/25720 [==============================] - 49s - loss: 0.9943 - categorical_accuracy: 0.5257    \n",
      "Epoch 14/100\n",
      "25720/25720 [==============================] - 48s - loss: 0.9878 - categorical_accuracy: 0.5275    \n",
      "Epoch 15/100\n",
      "25720/25720 [==============================] - 49s - loss: 0.9815 - categorical_accuracy: 0.5357    \n",
      "Epoch 16/100\n",
      "25720/25720 [==============================] - 49s - loss: 0.9738 - categorical_accuracy: 0.5432    \n",
      "Epoch 17/100\n",
      "25720/25720 [==============================] - 48s - loss: 0.9667 - categorical_accuracy: 0.5472    \n",
      "Epoch 18/100\n",
      "25720/25720 [==============================] - 48s - loss: 0.9563 - categorical_accuracy: 0.5551    \n",
      "Epoch 19/100\n",
      "25720/25720 [==============================] - 48s - loss: 0.9397 - categorical_accuracy: 0.5680    \n",
      "Epoch 20/100\n",
      "25720/25720 [==============================] - 48s - loss: 0.9096 - categorical_accuracy: 0.5872    \n",
      "Epoch 21/100\n",
      "25720/25720 [==============================] - 48s - loss: 0.8699 - categorical_accuracy: 0.6151    \n",
      "Epoch 22/100\n",
      "25720/25720 [==============================] - 48s - loss: 0.8479 - categorical_accuracy: 0.6286    \n",
      "Epoch 23/100\n",
      "25720/25720 [==============================] - 48s - loss: 0.8318 - categorical_accuracy: 0.6383    \n",
      "Epoch 24/100\n",
      "25720/25720 [==============================] - 48s - loss: 0.8207 - categorical_accuracy: 0.6438    \n",
      "Epoch 25/100\n",
      "25720/25720 [==============================] - 48s - loss: 0.8105 - categorical_accuracy: 0.6469    \n",
      "Epoch 26/100\n",
      "25720/25720 [==============================] - 48s - loss: 0.8039 - categorical_accuracy: 0.6547    \n",
      "Epoch 27/100\n",
      "25720/25720 [==============================] - 48s - loss: 0.7999 - categorical_accuracy: 0.6525    \n",
      "Epoch 28/100\n",
      "25720/25720 [==============================] - 48s - loss: 0.7944 - categorical_accuracy: 0.6554    \n",
      "Epoch 29/100\n",
      "25720/25720 [==============================] - 48s - loss: 0.7875 - categorical_accuracy: 0.6592    \n",
      "Epoch 30/100\n",
      "25720/25720 [==============================] - 48s - loss: 0.7799 - categorical_accuracy: 0.6639    \n",
      "Epoch 31/100\n",
      "25720/25720 [==============================] - 48s - loss: 0.7792 - categorical_accuracy: 0.6625    \n",
      "Epoch 32/100\n",
      "25720/25720 [==============================] - 48s - loss: 0.7736 - categorical_accuracy: 0.6661    \n",
      "Epoch 33/100\n",
      "25720/25720 [==============================] - 48s - loss: 0.7659 - categorical_accuracy: 0.6689    \n",
      "Epoch 34/100\n",
      "25720/25720 [==============================] - 48s - loss: 0.7639 - categorical_accuracy: 0.6714    \n",
      "Epoch 35/100\n",
      "25720/25720 [==============================] - 48s - loss: 0.7559 - categorical_accuracy: 0.6732    \n",
      "Epoch 36/100\n",
      "25720/25720 [==============================] - 48s - loss: 0.7521 - categorical_accuracy: 0.6739    \n",
      "Epoch 37/100\n",
      "25720/25720 [==============================] - 48s - loss: 0.7489 - categorical_accuracy: 0.6764    \n",
      "Epoch 38/100\n",
      "25720/25720 [==============================] - 48s - loss: 0.7448 - categorical_accuracy: 0.6781    \n",
      "Epoch 39/100\n",
      "25720/25720 [==============================] - 48s - loss: 0.7411 - categorical_accuracy: 0.6776    \n",
      "Epoch 40/100\n",
      "25720/25720 [==============================] - 48s - loss: 0.7375 - categorical_accuracy: 0.6840    \n",
      "Epoch 41/100\n",
      "25720/25720 [==============================] - 47s - loss: 0.7354 - categorical_accuracy: 0.6831    \n",
      "Epoch 42/100\n",
      "25720/25720 [==============================] - 40s - loss: 0.7291 - categorical_accuracy: 0.6852    \n",
      "Epoch 43/100\n",
      "25720/25720 [==============================] - 40s - loss: 0.7288 - categorical_accuracy: 0.6841    \n",
      "Epoch 44/100\n",
      "25720/25720 [==============================] - 40s - loss: 0.7245 - categorical_accuracy: 0.6839    \n",
      "Epoch 45/100\n",
      "25720/25720 [==============================] - 40s - loss: 0.7201 - categorical_accuracy: 0.6867     ETA: 1s - loss: 0.719\n",
      "Epoch 46/100\n",
      "25720/25720 [==============================] - 40s - loss: 0.7206 - categorical_accuracy: 0.6889    \n",
      "Epoch 47/100\n",
      "25720/25720 [==============================] - 40s - loss: 0.7129 - categorical_accuracy: 0.6913    \n",
      "Epoch 48/100\n",
      "25720/25720 [==============================] - 40s - loss: 0.7134 - categorical_accuracy: 0.6903    \n",
      "Epoch 49/100\n",
      "25720/25720 [==============================] - 39s - loss: 0.7081 - categorical_accuracy: 0.6930    \n",
      "Epoch 50/100\n",
      "25720/25720 [==============================] - 40s - loss: 0.7047 - categorical_accuracy: 0.6949     ETA: 2s -\n",
      "Epoch 51/100\n",
      "25720/25720 [==============================] - 40s - loss: 0.7034 - categorical_accuracy: 0.6950     E\n",
      "Epoch 52/100\n",
      "25720/25720 [==============================] - 40s - loss: 0.7008 - categorical_accuracy: 0.6995     ETA: 2s -\n",
      "Epoch 53/100\n",
      "25720/25720 [==============================] - 40s - loss: 0.6995 - categorical_accuracy: 0.6947    \n",
      "Epoch 54/100\n",
      "25720/25720 [==============================] - 40s - loss: 0.6932 - categorical_accuracy: 0.7003    \n",
      "Epoch 55/100\n",
      "25720/25720 [==============================] - 40s - loss: 0.6930 - categorical_accuracy: 0.6993     ETA: 1s - los\n",
      "Epoch 56/100\n",
      "25720/25720 [==============================] - 40s - loss: 0.6916 - categorical_accuracy: 0.6997    \n",
      "Epoch 57/100\n",
      "25720/25720 [==============================] - 40s - loss: 0.6900 - categorical_accuracy: 0.7000    \n",
      "Epoch 58/100\n",
      "25720/25720 [==============================] - 39s - loss: 0.6864 - categorical_accuracy: 0.7026    \n",
      "Epoch 59/100\n",
      "25720/25720 [==============================] - 39s - loss: 0.6863 - categorical_accuracy: 0.7008    \n",
      "Epoch 60/100\n",
      "25720/25720 [==============================] - 39s - loss: 0.6836 - categorical_accuracy: 0.7032     ETA: \n",
      "Epoch 61/100\n",
      "25720/25720 [==============================] - 40s - loss: 0.6810 - categorical_accuracy: 0.7015    \n",
      "Epoch 62/100\n",
      "25720/25720 [==============================] - 40s - loss: 0.6814 - categorical_accuracy: 0.7030    \n",
      "Epoch 63/100\n",
      "25720/25720 [==============================] - 40s - loss: 0.6769 - categorical_accuracy: 0.7044    \n",
      "Epoch 64/100\n",
      "25720/25720 [==============================] - 40s - loss: 0.6766 - categorical_accuracy: 0.7040    \n",
      "Epoch 65/100\n",
      "25720/25720 [==============================] - 39s - loss: 0.6761 - categorical_accuracy: 0.7052    \n",
      "Epoch 66/100\n",
      "25720/25720 [==============================] - 39s - loss: 0.6743 - categorical_accuracy: 0.7057    \n",
      "Epoch 67/100\n",
      "25720/25720 [==============================] - 39s - loss: 0.6706 - categorical_accuracy: 0.7074    \n",
      "Epoch 68/100\n",
      "25720/25720 [==============================] - 40s - loss: 0.6688 - categorical_accuracy: 0.7094    \n",
      "Epoch 69/100\n",
      "25720/25720 [==============================] - 39s - loss: 0.6683 - categorical_accuracy: 0.7100    \n",
      "Epoch 70/100\n",
      "25720/25720 [==============================] - 39s - loss: 0.6662 - categorical_accuracy: 0.7094    \n",
      "Epoch 71/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25720/25720 [==============================] - 39s - loss: 0.6636 - categorical_accuracy: 0.7101    \n",
      "Epoch 72/100\n",
      "25720/25720 [==============================] - 37s - loss: 0.6680 - categorical_accuracy: 0.7080    \n",
      "Epoch 73/100\n",
      "25720/25720 [==============================] - 38s - loss: 0.6633 - categorical_accuracy: 0.7094    \n",
      "Epoch 74/100\n",
      "25720/25720 [==============================] - 37s - loss: 0.6602 - categorical_accuracy: 0.7126     ETA: 1s - loss: 0.6598 - cate\n",
      "Epoch 75/100\n",
      "25720/25720 [==============================] - 37s - loss: 0.6610 - categorical_accuracy: 0.7114    \n",
      "Epoch 76/100\n",
      "25720/25720 [==============================] - 37s - loss: 0.6590 - categorical_accuracy: 0.7122    \n",
      "Epoch 77/100\n",
      "25720/25720 [==============================] - 37s - loss: 0.6574 - categorical_accuracy: 0.7112    \n",
      "Epoch 78/100\n",
      "25720/25720 [==============================] - 37s - loss: 0.6557 - categorical_accuracy: 0.7139    \n",
      "Epoch 79/100\n",
      "25720/25720 [==============================] - 39s - loss: 0.6540 - categorical_accuracy: 0.7127    \n",
      "Epoch 80/100\n",
      "25720/25720 [==============================] - 45s - loss: 0.6537 - categorical_accuracy: 0.7118    \n",
      "Epoch 81/100\n",
      "25720/25720 [==============================] - 49s - loss: 0.6532 - categorical_accuracy: 0.7142    \n",
      "Epoch 82/100\n",
      "25720/25720 [==============================] - 49s - loss: 0.6527 - categorical_accuracy: 0.7164    \n",
      "Epoch 83/100\n",
      "25720/25720 [==============================] - 49s - loss: 0.6528 - categorical_accuracy: 0.7159    \n",
      "Epoch 84/100\n",
      "25720/25720 [==============================] - 49s - loss: 0.6488 - categorical_accuracy: 0.7172    \n",
      "Epoch 85/100\n",
      "25720/25720 [==============================] - 46s - loss: 0.6487 - categorical_accuracy: 0.7177    \n",
      "Epoch 86/100\n",
      "25720/25720 [==============================] - 51s - loss: 0.6481 - categorical_accuracy: 0.7153    \n",
      "Epoch 87/100\n",
      "25720/25720 [==============================] - 50s - loss: 0.6446 - categorical_accuracy: 0.7200    \n",
      "Epoch 88/100\n",
      "25720/25720 [==============================] - 53s - loss: 0.6447 - categorical_accuracy: 0.7173    \n",
      "Epoch 89/100\n",
      "25720/25720 [==============================] - 49s - loss: 0.6449 - categorical_accuracy: 0.7182    \n",
      "Epoch 90/100\n",
      "25720/25720 [==============================] - 51s - loss: 0.6436 - categorical_accuracy: 0.7166    \n",
      "Epoch 91/100\n",
      "25720/25720 [==============================] - 43s - loss: 0.6422 - categorical_accuracy: 0.7173     ETA\n",
      "Epoch 92/100\n",
      "25720/25720 [==============================] - 40s - loss: 0.6408 - categorical_accuracy: 0.7180    \n",
      "Epoch 93/100\n",
      "25720/25720 [==============================] - 48s - loss: 0.6374 - categorical_accuracy: 0.7198    \n",
      "Epoch 94/100\n",
      "25720/25720 [==============================] - 51s - loss: 0.6396 - categorical_accuracy: 0.7184    \n",
      "Epoch 95/100\n",
      "25720/25720 [==============================] - 56s - loss: 0.6383 - categorical_accuracy: 0.7216    \n",
      "Epoch 96/100\n",
      "25720/25720 [==============================] - 50s - loss: 0.6343 - categorical_accuracy: 0.7229    \n",
      "Epoch 97/100\n",
      "25720/25720 [==============================] - 51s - loss: 0.6359 - categorical_accuracy: 0.7202    \n",
      "Epoch 98/100\n",
      "25720/25720 [==============================] - 52s - loss: 0.6332 - categorical_accuracy: 0.7246    \n",
      "Epoch 99/100\n",
      "25720/25720 [==============================] - 51s - loss: 0.6338 - categorical_accuracy: 0.7217    \n",
      "Epoch 100/100\n",
      "25720/25720 [==============================] - 51s - loss: 0.6323 - categorical_accuracy: 0.7250    \n"
     ]
    }
   ],
   "source": [
    "features = ['Volume_BTC', 'Bitcoin_Adj', 'Price_lagged']\n",
    "#features = ['Volume_BTC', 'Price_lagged']\n",
    "\n",
    "rnn = build_model(features, data_length) \n",
    "\n",
    "tensorboard_callback = TensorBoard(log_dir='./Graph', histogram_freq=0, write_graph=True, write_images=True)\n",
    "\n",
    "history = rnn.fit(\n",
    "    [\n",
    "        #X_train_timestamp,\n",
    "        X_train_volume,\n",
    "        X_train_trends,\n",
    "        X_train_lagged_price\n",
    "    ],\n",
    "    [\n",
    "        Y_train_is_spike_onehot\n",
    "    ]\n",
    "    ,\n",
    "#     validation_data=(\n",
    "#         [\n",
    "#             #X_test_timestamp,\n",
    "#             X_test_volume,\n",
    "#             #X_test_trends,\n",
    "#             X_test_lagged_price\n",
    "#         ],\n",
    "#         [\n",
    "#             Y_test_is_spike_onehot\n",
    "#         ]),\n",
    "    epochs=100,\n",
    "    batch_size=64,\n",
    "    callbacks=[\n",
    "      tensorboard_callback\n",
    "    ],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\niEgUUriLiEQhhbuISBRSuIuIRKH/D512Mci4sEL0AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1b4500640f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# plot history\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "#plt.plot(history.history['val_loss'], label='test')\n",
    "plt.legend()\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4448/4520 [============================>.] - ETA: 0s\n",
      "\n",
      "Accuracy: 46.90%\n"
     ]
    }
   ],
   "source": [
    "score = rnn.evaluate(\n",
    "    [\n",
    "        #X_test_timestamp,\n",
    "        X_test_volume,\n",
    "        X_test_trends,\n",
    "        X_test_lagged_price\n",
    "    ],\n",
    "    [\n",
    "        Y_test_is_spike_onehot\n",
    "    ])\n",
    "\n",
    "print('\\n')\n",
    "print(\"Accuracy: %.2f%%\" % (score[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To test if Google Trends actually have any benefit in predicting spikes, I ran one with and without the trend data as input. \n",
    "\n",
    "#### For \"Is Spike\" cutoff of 0.1, (meaning Is Spike marks only the 10% biggest changes)\n",
    "    * With trend data, accuracy was 78.90% on test data.\n",
    "    * Without trend data, accuracy was 82.93% on test data.\n",
    "    \n",
    "#### For \"Is Spike\" cutoff of 0.3, \n",
    "    * With trend data, accuracy was 84.57% for epoch=40 and 89.69% for epoch=60, and 87.98% for epoch=100\n",
    "    * Without trend data, accuracy was 78.40% for epoch=40 and 88.88% for epoch=60, and 93.60% for epoch=100\n",
    "    \n",
    "    \n",
    "    * Accuracy on test data is much better than that of train data \n",
    "        * -> could be because the test data is statistically different than train data \n",
    "            * which makese sense because test data is the real big spike \n",
    "                    * Since I am using 10 hours of data to predict the next hour, it would make sense that the accuracy is good during this time since this is the time that people were looking up Bitcoin and perhaps buying them a few hours later \n",
    "                * get more up-to-date data \n",
    "        * OR BECAUSE TRAIN AND TEST DATA SOMEHOW OVERLAPS?\n",
    "        \n",
    "#### With updated data\n",
    "    * With trend data, accuracy was 45.12% for epoch=100, 45.56% for epoch=200 -> overfitting?\n",
    "        * -> put in dropout \n",
    "    - Without trend data, accuracy was 43.11% for epoch=100, 43/97% for epoch=200\n",
    "    \n",
    "    * with the updated data, the test data is now from December 20th, which is right after the massive spike already happened\n",
    "    \n",
    "#### With Updated Data and with Dropout of 0.2 \n",
    "    * With trend data, accuracy was 38.61% for epoch=150\n",
    "    * Without trend data, accuracy was 38.87% for epoch=150\n",
    "    \n",
    "    TODO: Increase data_length (memory in LSTM) and change dropout \n",
    "    \n",
    "#### Increasing data_length to 20 (with dropout)\n",
    "    * With trend data, accuracy was 38.23% with epoch=150\n",
    "    \n",
    "#### Data_length of 20 without dropout \n",
    "    * With trend data, accuracy was 46.90% with epoch=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.02534344,  0.05034934,  0.92430729],\n",
       "       [ 0.03313668,  0.90681744,  0.06004593],\n",
       "       [ 0.43258792,  0.43481657,  0.1325956 ],\n",
       "       ..., \n",
       "       [ 0.37551153,  0.06019156,  0.56429696],\n",
       "       [ 0.53921092,  0.44531742,  0.01547169],\n",
       "       [ 0.17624743,  0.30557993,  0.51817256]], dtype=float32)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predicted</th>\n",
       "      <th>actual</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>-1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    predicted  actual\n",
       "0          -1    -1.0\n",
       "1           1     1.0\n",
       "2           1     1.0\n",
       "3          -1    -1.0\n",
       "4           1     1.0\n",
       "5           1     1.0\n",
       "6          -1    -1.0\n",
       "7          -1    -1.0\n",
       "8          -1    -1.0\n",
       "9          -1    -1.0\n",
       "10         -1    -1.0\n",
       "11          1     1.0\n",
       "12          1     1.0\n",
       "13         -1    -1.0\n",
       "14         -1    -1.0\n",
       "15          1     1.0\n",
       "16         -1    -1.0\n",
       "17          1     1.0\n",
       "18         -1    -1.0\n",
       "19          1     0.0\n",
       "20          1     1.0\n",
       "21          1     1.0\n",
       "22          1    -1.0\n",
       "23          1     1.0\n",
       "24          1     1.0\n",
       "25          1     1.0\n",
       "26         -1    -1.0\n",
       "27         -1    -1.0\n",
       "28         -1    -1.0\n",
       "29         -1    -1.0\n",
       "..        ...     ...\n",
       "70          1     1.0\n",
       "71          1     1.0\n",
       "72         -1     0.0\n",
       "73         -1    -1.0\n",
       "74         -1    -1.0\n",
       "75         -1    -1.0\n",
       "76          1     1.0\n",
       "77          1    -1.0\n",
       "78          1     1.0\n",
       "79          1     1.0\n",
       "80          1    -1.0\n",
       "81          1     1.0\n",
       "82          1    -1.0\n",
       "83          1     1.0\n",
       "84          1    -1.0\n",
       "85          1    -1.0\n",
       "86          1     1.0\n",
       "87          1    -1.0\n",
       "88          1     1.0\n",
       "89         -1    -1.0\n",
       "90         -1    -1.0\n",
       "91          1     1.0\n",
       "92          1     1.0\n",
       "93          1     1.0\n",
       "94          1     1.0\n",
       "95          1     1.0\n",
       "96          1     1.0\n",
       "97         -1    -1.0\n",
       "98          1     1.0\n",
       "99          1     1.0\n",
       "\n",
       "[100 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.45121412803532007"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "yhat = rnn.predict( \n",
    "    [\n",
    "        #X_test_timestamp,\n",
    "        X_test_volume,\n",
    "        X_test_trends,\n",
    "        X_test_lagged_price\n",
    "    ],\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "display(yhat)\n",
    "\n",
    "inverted_yhat = np.argmax(yhat,axis=1) #returns INDICES of max \n",
    "onehot_to_val_dict = {0: 0, 1: 1, 2:-1 }\n",
    "\n",
    "inverted_yhat_arr = np.asarray(inverted_yhat)\n",
    "predicted = [onehot_to_val_dict[i] for i in inverted_yhat_arr]\n",
    "\n",
    "\n",
    "df_pred_output = pd.DataFrame(predicted, columns=['predicted'])\n",
    "df_pred_output['actual'] = Y_test_is_spike\n",
    "#df_pred_output['index_output'] = inverted_yhat\n",
    "display(df_pred_output.head(100))\n",
    "\n",
    "correct = (df_pred_output['actual'].values == df_pred_output['predicted'].values)\n",
    "accuracy = correct.sum() / correct.size\n",
    "display(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# serialize model to JSON# serial \n",
    "model_json = rnn.to_json()\n",
    "with open(\"model_classification.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "    \n",
    "# serialize weights to HDF5\n",
    "rnn.save_weights(\"model_classification.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>-1</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>-1.0</th>\n",
       "      <td>808</td>\n",
       "      <td>298</td>\n",
       "      <td>628</td>\n",
       "      <td>1734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>333</td>\n",
       "      <td>476</td>\n",
       "      <td>382</td>\n",
       "      <td>1191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>500</td>\n",
       "      <td>345</td>\n",
       "      <td>760</td>\n",
       "      <td>1605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>1641</td>\n",
       "      <td>1119</td>\n",
       "      <td>1770</td>\n",
       "      <td>4530</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted    -1     0     1   All\n",
       "Actual                           \n",
       "-1.0        808   298   628  1734\n",
       "0.0         333   476   382  1191\n",
       "1.0         500   345   760  1605\n",
       "All        1641  1119  1770  4530"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "#print(metrics.confusion_matrix(df_pred_output['actual'].values, df_pred_output['predicted'].values,labels=[0,1,-1]))\n",
    "\n",
    "confusion_matrix = pd.crosstab(df_pred_output['actual'].values, df_pred_output['predicted'].values, rownames=['Actual'], colnames=['Predicted'], margins=True)\n",
    "\n",
    "display(confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# need to check what the rnn actually learned \n",
    "# visualize predicted vs actual to get insight into this \n",
    "\n",
    "# try with instead of just 10% biggest changes, maybe with 25% \n",
    "# is it just learning from the previous prices, or is google trends actually helping \n",
    "# -> run rnn without google trends \n",
    "\n",
    "\n",
    "# I have a master_df_v2 now so try that - this one has 0.3 as cutoff for is Spike \n",
    "# Have to eventually get validation data - also get overall newer more data "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
