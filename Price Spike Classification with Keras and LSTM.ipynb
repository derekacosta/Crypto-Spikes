{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from keras import Input\n",
    "from keras.engine import Model\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers import Concatenate, concatenate\n",
    "from keras.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# features is a list of strings of feature names \n",
    "\n",
    "def build_model(features, data_length):\n",
    "    \n",
    "    inputs_list = [] \n",
    "    for feature_name in features:\n",
    "        inputs_list.append((Input(shape=(data_length,1), name=feature_name)))\n",
    "    \n",
    "    layers = [] \n",
    "    for i, input_name in enumerate(inputs_list): \n",
    "        layers.append(LSTM(64, return_sequences=False)(inputs_list[i]) )\n",
    "        \n",
    "    output = concatenate(layers) \n",
    "    output = Dense(3, activation='softmax', name='IsSpike')(output)\n",
    "    \n",
    "    model = Model(\n",
    "        inputs = inputs_list,\n",
    "        outputs = [output]\n",
    "    )\n",
    "    \n",
    "    model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "    \n",
    "    return model    \n",
    "\n",
    "data_length = 20\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Shape\n",
    "\n",
    "* Price  ----------> LSTM --\\\n",
    "* Google Trends ---> LSTM ---> Dense Layer -> Softmax -> Output: Is Spike (1,0,-1) \n",
    "* Volume ----------> LSTM --/\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "* Input: Price, Google Trends, and Volume for time t0-t9 (10 hours of data) \n",
    "* Output: Is Spike (1,0,-1) for t10 \n",
    "    * Using 10 hours (t0-t9) of Price, Google Trends, and Volume to predict the price movement at t11 (t10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Shoya\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\__main__.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\Shoya\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\__main__.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\Shoya\\Anaconda2\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\__main__.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Volume_BTC</th>\n",
       "      <th>Bitcoin_Adj</th>\n",
       "      <th>Close</th>\n",
       "      <th>Price_lagged</th>\n",
       "      <th>Is Spike</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.776791</td>\n",
       "      <td>0.471970</td>\n",
       "      <td>0.484557</td>\n",
       "      <td>0.484557</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.463316</td>\n",
       "      <td>0.439996</td>\n",
       "      <td>0.538331</td>\n",
       "      <td>0.538331</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.725079</td>\n",
       "      <td>0.529463</td>\n",
       "      <td>0.520715</td>\n",
       "      <td>0.520715</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.210661</td>\n",
       "      <td>0.416611</td>\n",
       "      <td>0.566098</td>\n",
       "      <td>0.566098</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.594148</td>\n",
       "      <td>0.445509</td>\n",
       "      <td>0.568881</td>\n",
       "      <td>0.568881</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Volume_BTC  Bitcoin_Adj     Close  Price_lagged  Is Spike\n",
       "1    0.776791     0.471970  0.484557      0.484557       1.0\n",
       "2    0.463316     0.439996  0.538331      0.538331       1.0\n",
       "3    0.725079     0.529463  0.520715      0.520715      -1.0\n",
       "4    0.210661     0.416611  0.566098      0.566098       0.0\n",
       "5    0.594148     0.445509  0.568881      0.568881      -1.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Volume_BTC</th>\n",
       "      <th>Bitcoin_Adj</th>\n",
       "      <th>Close</th>\n",
       "      <th>Price_lagged</th>\n",
       "      <th>Is Spike</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>30285</th>\n",
       "      <td>0.455905</td>\n",
       "      <td>0.449846</td>\n",
       "      <td>0.546519</td>\n",
       "      <td>0.546519</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30286</th>\n",
       "      <td>0.308749</td>\n",
       "      <td>0.457405</td>\n",
       "      <td>0.549938</td>\n",
       "      <td>0.549938</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30287</th>\n",
       "      <td>0.632915</td>\n",
       "      <td>0.453625</td>\n",
       "      <td>0.563428</td>\n",
       "      <td>0.563428</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30288</th>\n",
       "      <td>0.629822</td>\n",
       "      <td>0.434124</td>\n",
       "      <td>0.506657</td>\n",
       "      <td>0.506657</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30289</th>\n",
       "      <td>0.564642</td>\n",
       "      <td>0.445374</td>\n",
       "      <td>0.493535</td>\n",
       "      <td>0.493535</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Volume_BTC  Bitcoin_Adj     Close  Price_lagged  Is Spike\n",
       "30285    0.455905     0.449846  0.546519      0.546519       1.0\n",
       "30286    0.308749     0.457405  0.549938      0.549938      -1.0\n",
       "30287    0.632915     0.453625  0.563428      0.563428       0.0\n",
       "30288    0.629822     0.434124  0.506657      0.506657       0.0\n",
       "30289    0.564642     0.445374  0.493535      0.493535      -1.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "master_df = pd.read_csv('C:/Users/Shoya/surf/data/master_df_v3.csv', encoding='latin1')\n",
    "df = master_df[['Timestamp', 'Close', 'Volume_(BTC)', 'Volume_(Currency)', 'Date(UTC)', 'Bitcoin (Adj.Overlap)', \n",
    "               'Close Price % Change', 'Close Price % Change (Abs)', 'Is Spike']]\n",
    "\n",
    "# lag inputs depending on data_length \n",
    "df['Price_lagged'] = df['Close']#.shift(data_length)\n",
    "df['Volume_BTC'] = df['Volume_(BTC)']#.shift(data_length)\n",
    "df['Bitcoin_Adj'] = df['Bitcoin (Adj.Overlap)']#.shift(data_length)\n",
    "\n",
    "df = df.dropna()\n",
    "cols = ['Volume_BTC','Bitcoin_Adj', 'Close', 'Price_lagged']\n",
    "\n",
    "# Stationalize Data by taking log differences\n",
    "data_array = np.diff(np.log(df[cols]), axis=0)\n",
    "\n",
    "# Min-Max Scale \n",
    "\n",
    "scalers = {}\n",
    "datas = [] \n",
    "\n",
    "df_scaled = pd.DataFrame(columns=cols)\n",
    "\n",
    "\n",
    "############################################################\n",
    "#  Fix below - I am scaling the whole data set together, when I should scale the train and test datasets separately\n",
    "############################################################\n",
    "\n",
    "for i in range(len(cols)): \n",
    "    scalers[cols[i]] = MinMaxScaler()\n",
    "    #print('data', data_array[:,i])\n",
    "    \n",
    "    col_data = data_array[:,i]\n",
    "    col_data = np.reshape(col_data, (len(col_data), 1))\n",
    "    \n",
    "    data = scalers[cols[i]].fit_transform( col_data )\n",
    "    #print('scaled', data)\n",
    "    data = np.reshape(data, (1, len(data)))\n",
    "    df_scaled[cols[i]] = data[0]\n",
    "    \n",
    "df_scaled['Is Spike'] = df['Is Spike']\n",
    "df_scaled.dropna(inplace=True)\n",
    "display(df_scaled.head())\n",
    "display(df_scaled.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0., -1.,  1., ...,  1.,  1., -1.])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "# split and reshape data to feed into RNN\n",
    "\n",
    "# X_timestamp = df_scaled['Timestamp'].values\n",
    "X_volume = df_scaled['Volume_BTC'].values\n",
    "X_trends = df_scaled['Bitcoin_Adj'].values\n",
    "X_lagged_price = df_scaled['Price_lagged'].values\n",
    "\n",
    "Y_is_spike = df_scaled['Is Spike'].values \n",
    "\n",
    "train_size = int(len(X_volume) * 0.85)\n",
    "train_size = int(train_size/data_length) * data_length\n",
    "\n",
    "test_size_index = int(len(X_volume)/data_length)*data_length\n",
    "\n",
    "X_train_volume = []\n",
    "X_test_volume = [] \n",
    "X_train_trends = []\n",
    "X_test_trends = []\n",
    "X_train_lagged_price = []\n",
    "X_test_lagged_price = []\n",
    "Y_train_is_spike = [] \n",
    "Y_test_is_spike = [] \n",
    "\n",
    "for i in range(train_size-data_length):\n",
    "    vol_temp = []\n",
    "    trends_temp = []\n",
    "    price_temp = []\n",
    "    for j in range(data_length):\n",
    "        vol_temp.append(X_volume[i+j])\n",
    "        trends_temp.append(X_trends[i+j])\n",
    "        price_temp.append(X_lagged_price[i+j])\n",
    "    X_train_volume.append(vol_temp)\n",
    "    X_train_trends.append(trends_temp)\n",
    "    X_train_lagged_price.append(price_temp)\n",
    "    \n",
    "    Y_train_is_spike.append(Y_is_spike[i+data_length])\n",
    "\n",
    "for i in range(test_size_index-train_size-data_length):\n",
    "    vol_temp = []\n",
    "    trends_temp = [] \n",
    "    price_temp = [] \n",
    "    for j in range(data_length):\n",
    "        vol_temp.append(X_volume[train_size+i+j])\n",
    "        trends_temp.append(X_trends[train_size+i+j])\n",
    "        price_temp.append(X_lagged_price[train_size+i+j])\n",
    "    X_test_volume.append(vol_temp)\n",
    "    X_test_trends.append(trends_temp)\n",
    "    X_test_lagged_price.append(price_temp)\n",
    "    \n",
    "    Y_test_is_spike.append(Y_is_spike[train_size+i+data_length])\n",
    "    \n",
    "X_train_volume = np.array(X_train_volume)\n",
    "X_test_volume =  np.array(X_test_volume)\n",
    "X_train_trends = np.array(X_train_trends)\n",
    "X_test_trends = np.array(X_test_trends)\n",
    "X_train_lagged_price = np.array(X_train_lagged_price)\n",
    "X_test_lagged_price = np.array(X_test_lagged_price)\n",
    "Y_train_is_spike =  np.array(Y_train_is_spike)\n",
    "Y_test_is_spike = np.array(Y_test_is_spike)\n",
    "    \n",
    "    \n",
    "Y_train_is_spike_onehot = to_categorical(Y_train_is_spike, num_classes=3)\n",
    "Y_test_is_spike_onehot = to_categorical(Y_test_is_spike,num_classes=3)\n",
    "display(Y_train_is_spike)\n",
    "\n",
    "# y = pd.DataFrame(Y_train_is_spike_onehot)\n",
    "# y['actual'] = Y_train_is_spike\n",
    "# display(y.head(25))\n",
    "    \n",
    "# display(X_train_trends.shape)\n",
    "# display(Y_train_is_spike.shape)\n",
    "\n",
    "#display(X_train_lagged_price)\n",
    "#display(Y_train_is_spike)\n",
    "\n",
    "# df_train = pd.DataFrame(X_train_lagged_price)\n",
    "# df_train['label'] = Y_train_is_spike\n",
    "# display(df_train.tail(20))\n",
    "# display(df_scaled.head(30))\n",
    "# display(df_train.head(30))\n",
    "\n",
    "#--------------------------------\n",
    "\n",
    "# # X_train_timestamp, X_test_timestamp = X_timestamp[:train_size], X_timestamp[train_size:test_size_index ]\n",
    "# X_train_volume, X_test_volume = X_volume[:train_size], X_volume[train_size:test_size_index ]\n",
    "# X_train_trends, X_test_trends = X_trends[:train_size], X_trends[train_size:test_size_index ]\n",
    "# X_train_lagged_price, X_test_lagged_price = X_lagged_price[:train_size], X_lagged_price[train_size:test_size_index ]\n",
    "\n",
    "# # becasue I lagged the x inputs, I should forward the Y's by the data_length as well \n",
    "# Y_train_is_spike, Y_test_is_spike = Y_is_spike[data_length:train_size], Y_is_spike[train_size+data_length:test_size_index ]\n",
    "\n",
    "\n",
    "# # X.shape is (samples, timesteps, dimension) \n",
    "# # timestemps is 15, samples is just however many nobs there are (but it doesn't matter, so it should be None)\n",
    "\n",
    "\n",
    "X_train_volume = np.reshape(X_train_volume, (X_train_volume.shape[0],data_length,1) ) \n",
    "X_train_trends = np.reshape(X_train_trends, (X_train_trends.shape[0],data_length,1) ) \n",
    "X_train_lagged_price = np.reshape(X_train_lagged_price, (X_train_lagged_price.shape[0], data_length, 1))\n",
    "\n",
    "X_test_volume = np.reshape(X_test_volume, (X_test_volume.shape[0],data_length,1) ) \n",
    "X_test_trends = np.reshape(X_test_trends, (X_test_trends.shape[0],data_length,1) )  \n",
    "X_test_lagged_price = np.reshape(X_test_lagged_price, (X_test_lagged_price.shape[0],data_length,1))\n",
    "\n",
    "\n",
    "# # X_train_timestamp = np.reshape(X_train_timestamp, (int(X_train_timestamp.shape[0]/data_length),data_length,1) ) \n",
    "# X_train_volume = np.reshape(X_train_volume, (int(X_train_volume.shape[0]/data_length),data_length,1) ) \n",
    "# X_train_trends = np.reshape(X_train_trends, (int(X_train_trends.shape[0]/data_length),data_length,1) ) \n",
    "# X_train_lagged_price = np.reshape(X_train_lagged_price, (int(X_train_lagged_price.shape[0]/data_length), data_length, 1))\n",
    "\n",
    "# # X_test_timestamp = np.reshape(X_test_timestamp, (int(X_test_timestamp.shape[0]/data_length),data_length,1) ) \n",
    "# X_test_volume = np.reshape(X_test_volume, (int(X_test_volume.shape[0]/data_length),data_length,1) ) \n",
    "# X_test_trends = np.reshape(X_test_trends, (int(X_test_trends.shape[0]/data_length),data_length,1) )  \n",
    "# X_test_lagged_price = np.reshape(X_test_lagged_price, (int(X_test_lagged_price.shape[0]/data_length),data_length,1))\n",
    "\n",
    "\n",
    "# # Don't need the 1 for the third dimension for Y's??\n",
    "\n",
    "\n",
    "# Y_train_is_spike = np.reshape(Y_train_is_spike, (int(Y_train_is_spike.shape[0]/data_length),  data_length) ) \n",
    "# Y_test_is_spike = np.reshape(Y_test_is_spike, (int(Y_test_is_spike.shape[0]/data_length),  data_length) )\n",
    "\n",
    "#-----------------------------------\n",
    "\n",
    "\n",
    "# instead of using input 1,2,3,4,5,6,7,8,9,10 to predict output for 11,12,13,14,15,16,17,18,19,20\n",
    "# I want to use input 1,2,3,4,5,6,7,8,9,10 to predict output for 11, then 2,3,4,5,6,7,8,9,10,11 to predict output for 12 \n",
    "\n",
    "# right now I am actually feeding input 1,2,3,4,5,6,7,8,9,10 to predict output for 1,2,3,4,5,6,7,8,9,10. \n",
    "# instead I should at least feed 1,2,3..8,9,10 to predict 11,12,13,14,15,16,17,18,19,20 -> lag everything by data_length! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "features = ['Volume_BTC', 'Bitcoin_Adj', 'Price_lagged']\n",
    "#features = ['Volume_BTC', 'Price_lagged']\n",
    "\n",
    "rnn = build_model(features, data_length) \n",
    "\n",
    "tensorboard_callback = TensorBoard(log_dir='./Graph', histogram_freq=0, write_graph=True, write_images=True)\n",
    "\n",
    "history = rnn.fit(\n",
    "    [\n",
    "        #X_train_timestamp,\n",
    "        X_train_volume,\n",
    "        X_train_trends,\n",
    "        X_train_lagged_price\n",
    "    ],\n",
    "    [\n",
    "        Y_train_is_spike_onehot\n",
    "    ]\n",
    "    ,\n",
    "#     validation_data=(\n",
    "#         [\n",
    "#             #X_test_timestamp,\n",
    "#             X_test_volume,\n",
    "#             #X_test_trends,\n",
    "#             X_test_lagged_price\n",
    "#         ],\n",
    "#         [\n",
    "#             Y_test_is_spike_onehot\n",
    "#         ]),\n",
    "    epochs=100,\n",
    "    batch_size=64,\n",
    "    callbacks=[\n",
    "      tensorboard_callback\n",
    "    ],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD8CAYAAACb4nSYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1cd4182b630>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# plot history\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "#plt.plot(history.history['val_loss'], label='test')\n",
    "plt.legend()\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4448/4520 [============================>.] - ETA: 0s\n",
      "\n",
      "Accuracy: 38.23%\n"
     ]
    }
   ],
   "source": [
    "score = rnn.evaluate(\n",
    "    [\n",
    "        #X_test_timestamp,\n",
    "        X_test_volume,\n",
    "        X_test_trends,\n",
    "        X_test_lagged_price\n",
    "    ],\n",
    "    [\n",
    "        Y_test_is_spike_onehot\n",
    "    ])\n",
    "\n",
    "print('\\n')\n",
    "print(\"Accuracy: %.2f%%\" % (score[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To test if Google Trends actually have any benefit in predicting spikes, I ran one with and without the trend data as input. \n",
    "\n",
    "#### For \"Is Spike\" cutoff of 0.1, (meaning Is Spike marks only the 10% biggest changes)\n",
    "    * With trend data, accuracy was 78.90% on test data.\n",
    "    * Without trend data, accuracy was 82.93% on test data.\n",
    "    \n",
    "#### For \"Is Spike\" cutoff of 0.3, \n",
    "    * With trend data, accuracy was 84.57% for epoch=40 and 89.69% for epoch=60, and 87.98% for epoch=100\n",
    "    * Without trend data, accuracy was 78.40% for epoch=40 and 88.88% for epoch=60, and 93.60% for epoch=100\n",
    "    \n",
    "    Thus Google Trends actually helped Is Spike in the latter case!!!! \n",
    "    \n",
    "    \n",
    "    * Accuracy on test data is much better than that of train data \n",
    "        * -> could be because the test data is statistically different than train data \n",
    "            * which makese sense because test data is the real big spike \n",
    "                    * Since I am using 10 hours of data to predict the next hour, it would make sense that the accuracy is good during this time since this is the time that people were looking up Bitcoin and perhaps buying them a few hours later \n",
    "                * get more up-to-date data \n",
    "        * OR BECAUSE TRAIN AND TEST DATA SOMEHOW OVERLAPS?\n",
    "        \n",
    "#### With updated data\n",
    "    * With trend data, accuracy was 45.12% for epoch=100, 45.56% for epoch=200 -> overfitting?\n",
    "        * -> put in dropout \n",
    "    - Without trend data, accuracy was 43.11% for epoch=100, 43/97% for epoch=200\n",
    "    \n",
    "    * with the updated data, the test data is now from December 20th, which is right after the massive spike already happened\n",
    "    \n",
    "#### With Updated Data and with Dropout of 0.2 \n",
    "    * With trend data, accuracy was 38.61% for epoch=150\n",
    "    * Without trend data, accuracy was 38.87% for epoch=150\n",
    "    \n",
    "    TODO: Increase data_length (memory in LSTM) and change dropout \n",
    "    \n",
    "#### Increasing data_length to 20 (with dropout)\n",
    "    * With trend data, accuracy was 38.23% with epoch=150\n",
    "    \n",
    "#### Data_length of 20 without dropout \n",
    "    * With trend data, accuracy was X% with epoch=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.02534344,  0.05034934,  0.92430729],\n",
       "       [ 0.03313668,  0.90681744,  0.06004593],\n",
       "       [ 0.43258792,  0.43481657,  0.1325956 ],\n",
       "       ..., \n",
       "       [ 0.37551153,  0.06019156,  0.56429696],\n",
       "       [ 0.53921092,  0.44531742,  0.01547169],\n",
       "       [ 0.17624743,  0.30557993,  0.51817256]], dtype=float32)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predicted</th>\n",
       "      <th>actual</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>-1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>1</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>-1</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    predicted  actual\n",
       "0          -1    -1.0\n",
       "1           1     1.0\n",
       "2           1     1.0\n",
       "3          -1    -1.0\n",
       "4           1     1.0\n",
       "5           1     1.0\n",
       "6          -1    -1.0\n",
       "7          -1    -1.0\n",
       "8          -1    -1.0\n",
       "9          -1    -1.0\n",
       "10         -1    -1.0\n",
       "11          1     1.0\n",
       "12          1     1.0\n",
       "13         -1    -1.0\n",
       "14         -1    -1.0\n",
       "15          1     1.0\n",
       "16         -1    -1.0\n",
       "17          1     1.0\n",
       "18         -1    -1.0\n",
       "19          1     0.0\n",
       "20          1     1.0\n",
       "21          1     1.0\n",
       "22          1    -1.0\n",
       "23          1     1.0\n",
       "24          1     1.0\n",
       "25          1     1.0\n",
       "26         -1    -1.0\n",
       "27         -1    -1.0\n",
       "28         -1    -1.0\n",
       "29         -1    -1.0\n",
       "..        ...     ...\n",
       "70          1     1.0\n",
       "71          1     1.0\n",
       "72         -1     0.0\n",
       "73         -1    -1.0\n",
       "74         -1    -1.0\n",
       "75         -1    -1.0\n",
       "76          1     1.0\n",
       "77          1    -1.0\n",
       "78          1     1.0\n",
       "79          1     1.0\n",
       "80          1    -1.0\n",
       "81          1     1.0\n",
       "82          1    -1.0\n",
       "83          1     1.0\n",
       "84          1    -1.0\n",
       "85          1    -1.0\n",
       "86          1     1.0\n",
       "87          1    -1.0\n",
       "88          1     1.0\n",
       "89         -1    -1.0\n",
       "90         -1    -1.0\n",
       "91          1     1.0\n",
       "92          1     1.0\n",
       "93          1     1.0\n",
       "94          1     1.0\n",
       "95          1     1.0\n",
       "96          1     1.0\n",
       "97         -1    -1.0\n",
       "98          1     1.0\n",
       "99          1     1.0\n",
       "\n",
       "[100 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.45121412803532007"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "yhat = rnn.predict( \n",
    "    [\n",
    "        #X_test_timestamp,\n",
    "        X_test_volume,\n",
    "        X_test_trends,\n",
    "        X_test_lagged_price\n",
    "    ],\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "display(yhat)\n",
    "\n",
    "inverted_yhat = np.argmax(yhat,axis=1) #returns INDICES of max \n",
    "onehot_to_val_dict = {0: 0, 1: 1, 2:-1 }\n",
    "\n",
    "inverted_yhat_arr = np.asarray(inverted_yhat)\n",
    "predicted = [onehot_to_val_dict[i] for i in inverted_yhat_arr]\n",
    "\n",
    "\n",
    "df_pred_output = pd.DataFrame(predicted, columns=['predicted'])\n",
    "df_pred_output['actual'] = Y_test_is_spike\n",
    "#df_pred_output['index_output'] = inverted_yhat\n",
    "display(df_pred_output.head(100))\n",
    "\n",
    "correct = (df_pred_output['actual'].values == df_pred_output['predicted'].values)\n",
    "accuracy = correct.sum() / correct.size\n",
    "display(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# serialize model to JSON# serial \n",
    "model_json = rnn.to_json()\n",
    "with open(\"model_classification.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "    \n",
    "# serialize weights to HDF5\n",
    "rnn.save_weights(\"model_classification.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>-1</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>-1.0</th>\n",
       "      <td>808</td>\n",
       "      <td>298</td>\n",
       "      <td>628</td>\n",
       "      <td>1734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>333</td>\n",
       "      <td>476</td>\n",
       "      <td>382</td>\n",
       "      <td>1191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>500</td>\n",
       "      <td>345</td>\n",
       "      <td>760</td>\n",
       "      <td>1605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>1641</td>\n",
       "      <td>1119</td>\n",
       "      <td>1770</td>\n",
       "      <td>4530</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted    -1     0     1   All\n",
       "Actual                           \n",
       "-1.0        808   298   628  1734\n",
       "0.0         333   476   382  1191\n",
       "1.0         500   345   760  1605\n",
       "All        1641  1119  1770  4530"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "#print(metrics.confusion_matrix(df_pred_output['actual'].values, df_pred_output['predicted'].values,labels=[0,1,-1]))\n",
    "\n",
    "confusion_matrix = pd.crosstab(df_pred_output['actual'].values, df_pred_output['predicted'].values, rownames=['Actual'], colnames=['Predicted'], margins=True)\n",
    "\n",
    "display(confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# need to check what the rnn actually learned \n",
    "# visualize predicted vs actual to get insight into this \n",
    "\n",
    "# try with instead of just 10% biggest changes, maybe with 25% \n",
    "# is it just learning from the previous prices, or is google trends actually helping \n",
    "# -> run rnn without google trends \n",
    "\n",
    "\n",
    "# I have a master_df_v2 now so try that - this one has 0.3 as cutoff for is Spike \n",
    "# Have to eventually get validation data - also get overall newer more data "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
