{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To test if Google Trends actually have any benefit in predicting spikes, I ran one with and without the trend data as input. \n",
    "\n",
    "#### For \"Is Spike\" cutoff of 0.1, (meaning Is Spike marks only the 10% biggest changes)\n",
    "    * With trend data, accuracy was 78.90% on test data.\n",
    "    * Without trend data, accuracy was 82.93% on test data.\n",
    "    \n",
    "#### For \"Is Spike\" cutoff of 0.3, \n",
    "    * With trend data, accuracy was 84.57% for epoch=40 and 89.69% for epoch=60, 87.98% for epoch=100, and 92.02% for epoch=200\n",
    "    * Without trend data, accuracy was 78.40% for epoch=40 and 88.88% for epoch=60, 93.60% for epoch=100, and 90.14% for epoch=200\n",
    "    \n",
    "    \n",
    "    * Accuracy on test data is much better than that of train data \n",
    "        * -> could be because the test data is statistically different than train data \n",
    "            * which makese sense because test data is the real big spike \n",
    "                    * Since I am using 10 hours of data to predict the next hour, it would make sense that the accuracy is good during this time since this is the time that people were looking up Bitcoin and perhaps buying them a few hours later \n",
    "                * get more up-to-date data \n",
    "        * OR BECAUSE TRAIN AND TEST DATA SOMEHOW OVERLAPS?\n",
    "        \n",
    "#### With updated data\n",
    "    * With trend data, accuracy was 45.12% for epoch=100, 45.56% for epoch=200 -> overfitting?\n",
    "        * -> put in dropout \n",
    "    - Without trend data, accuracy was 43.11% for epoch=100, 43.97% for epoch=200\n",
    "    \n",
    "    * with the updated data, the test data is now from December 20th, which is right after the massive spike already happened, to June\n",
    "    * Before, test data used to be from start of October to April-ish\n",
    "    \n",
    "#### With Updated Data and with Dropout of 0.2 \n",
    "    * With trend data, accuracy was 38.61% for epoch=150\n",
    "    * Without trend data, accuracy was 38.87% for epoch=150\n",
    "    \n",
    "    TODO: Increase data_length (memory in LSTM) and change dropout \n",
    "    \n",
    "#### Data_length=12 with Dropout of 0.2 \n",
    "    * With trend data, accuracy was 38.44% for epoch=400\n",
    "    * Without trend data, accuracy was 31.90% for epoch=400\n",
    "    \n",
    "#### Increasing data_length to 20 (with dropout)\n",
    "    * With trend data, accuracy was 38.23% with epoch=150\n",
    "    \n",
    "#### Data_length of 20 without dropout \n",
    "    * With trend data, accuracy was 46.90% with epoch=100\n",
    "    * Without trend data, accuracy was 46.13% with epoch=100\n",
    "    \n",
    "#### Data_length of 24 without dropout\n",
    "    * With trend data, accuracy was 44.17% with epoch=200\n",
    "    \n",
    "#### Data_length of 35 without dropout\n",
    "    * With trend data, accuracy was 46.8% with epoch=100\n",
    "    * Without trend data, accuracy was 43.52% with epoch=100\n",
    "    \n",
    "#### Data_length of 50 without dropout \n",
    "    * With trend data, accuracy was 45.40% with epoch=100\n",
    "    * Without trend data, accuracy was 45.67% with epoch=100\n",
    "    \n",
    "### After introducing validation data and changing training to be 80% instead of 85% \n",
    "    \n",
    "#### Data_length of 12 with dropout=0.2 \n",
    "    * With trend data, accuracy was 37.40% for epoch = 25, 37.95% for epoch=200\n",
    "    * Without trend data, accuracy was 36.94% for epoch=25 X% for epoch=200\n",
    "        * -> Overfit way too much and the validation loss was actually increasing a lot \n",
    "            * -> that's probably what's been happening for all\n",
    "\n",
    "### After moving around train/validation/train \n",
    "![image.png](attachment:image.png) 24516 27252 30284\n",
    "\n",
    "#### Data_length of 12 with dropout=0.2 \n",
    "    * With trend data, accuracy was 34.69% for epoch=100 \n",
    "    * Without trend data, accuracy was 33.86% for epoch=100\n",
    "  \n",
    "    \n",
    "### After moving around train/validation/train again \n",
    "![image.png](attachment:image.png) 25884 27252 30284\n",
    "\n",
    "#### Data_length of 12 with dropout=0.2\n",
    "   * With trend data, accuracy was 34.79% for epoch=100 \n",
    "   * Without trend data, accuracy was 34.73% for epoch=100\n",
    "    \n",
    "#### data_length of 12 without dropout\n",
    "    * With trend data, accuracy was 37.38% for epoch=30, 31.97% for epoch=100\n",
    "    * Without trend data, accuracy was 35.86% for epoch=30, 32.34% for epoch=100\n",
    "    \n",
    "### Getting rid of Validation data and going back to 90/10 train/test split\n",
    "\n",
    "#### Data_length of 12 without dropout\n",
    "    * With trend data, accuracy was 42.43% for epoch=10, 43.63% for epoch=20, 38.68% for epoch=35, 32.34% for epoch=100\n",
    "    * Without trend data, accuracy was 43.59% for epoch=10, 44.85% for epoch=20, 34.13% for epoch=35, 32.70% for epoch=100\n",
    "    \n",
    "### With 80/20 train/test split\n",
    "\n",
    "### Data_length of 12 without dropout \n",
    "    * With trend data for \"bitcoin\", accuracy was 48.51% for epoch=25, 56.60% for epoch=100, 54.50% for epoch=200\n",
    "    * With trend data for \"Coinbase\", accuracy was 51.01% for epoch=30, 55% for epoch=100\n",
    "    * Without trend data, accuracy was 52.41% for epoch=25, 57.21% for epoch=100, 53.64% for epoch=200\n",
    " \n",
    "### Data_length of 60 without dropout\n",
    "    * With trend data for \"Coinbase\", accuracy was 51.21% for epoch=50\n",
    "    \n",
    "## Add in TimeDistributed Dense and another Dense Layer\n",
    "\n",
    "### data length of 24 with Coinbase\n",
    "    * With trend data for \"Coinbase,\" accuracy was 53.28% for epoch=25, 58.17% for epoch=100\n",
    "    * Without trend data, accuracy was 52.72% for epoch=25, 55.12% for epoch=100  \n",
    "    \n",
    "## Changed Cells of LSTM to 32\n",
    "\n",
    "### With data length of 24\n",
    "    * With trend data for Coinbase, Blockchain, Bubble, and Bitcoin, accuracy was 55.02% for epoch=50\n",
    "\n",
    "### With data length of 64 \n",
    "    * With trend data for Coinbase, Blockchain, Bubble, and Bitcoin, accuracy was 54.02% for epoch=50\n",
    "    \n",
    "\n",
    "## Changed Cells of LSTM to 32\n",
    "\n",
    "### With data length of 24\n",
    "    * With trend data for all, accuracy was 56.78% for epoch=50 and 58.15% for epoch=100 (maybe do more) \n",
    "    \n",
    "    \n",
    "## Changed Cells of LSTM to 16\n",
    "\n",
    "### With data length of 64 \n",
    "    * With trend data for all, accuracy was 52.81% for epoch=50\n",
    "\n",
    "\n",
    "* TODO: add dropout?? \n",
    "\n",
    "    \n",
    "* I can also add time distributed dense layer after each lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
